[
  {
    "objectID": "beyond-dataframes.html#overview",
    "href": "beyond-dataframes.html#overview",
    "title": "1  Beyond Dataframes",
    "section": "\n1.1 Overview",
    "text": "1.1 Overview\nIn this mini course we will be exploring the use of computing and non-parametric method in statistical practice. This will look like a set of general programming and computing topics and methods. This will require that we use a statistical programming language in sophisticated ways, and the module will specifically explore techniques for advanced programming in R.\nReadings come from the Advanced R textbook by Hadley Wickham.\n\n``To understand computations in R, two slogans are helpful: Everything that exists is an object. Everything that happens is a function call.’’ — John Chambers\n\nYou already know the basics from your previous coursework. Here we will briefly discuss the following:\n\n\nDocumenting Code\n\nNaming objects, functions, variables\nDescriptive comments\nStructural organization\n\n\n\nObjects\n\nVectors, atomic types, attributes\nDataframes\nMatrices\n\n\n\nFunctions\n\nUsing existing functions\nFinding and reading help documentation",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Beyond Dataframes</span>"
    ]
  },
  {
    "objectID": "beyond-dataframes.html#coding-in-r---documenting-with-quarto",
    "href": "beyond-dataframes.html#coding-in-r---documenting-with-quarto",
    "title": "1  Beyond Dataframes",
    "section": "\n1.2 Coding in R - Documenting with Quarto",
    "text": "1.2 Coding in R - Documenting with Quarto\nIn this mini-course, we will be coding and programming using the R language. We will be leveraging the benefits of the open source structure of R by using some of the excellent software packages developed by members of the R community. We will also be exploring the efficient data structures and functions available in Base R. We will be interacting with the R language using the R Studio Interactive Development Environment (IDE).\nWe will be using Quarto to document our work because it is capable of integrating the process of writing in both programming and natural languages. It is great to be able to transition back and forth between paragraphs expressing what we are trying to accomplish statistically to the computational evidence to support our results. Each course notes will be put together as a Quarto document.\nSupplementary Resources for R Markdown:\n\nQuarto Website\nQuarto computations in R\nR Markdown Reference Guide",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Beyond Dataframes</span>"
    ]
  },
  {
    "objectID": "beyond-dataframes.html#r-basics",
    "href": "beyond-dataframes.html#r-basics",
    "title": "1  Beyond Dataframes",
    "section": "\n1.3 R Basics",
    "text": "1.3 R Basics\nData structures\n\nVectors are the atomic vectors of type: numeric, character, logical\nEssentially every data structure we use is one of these vectors, but with additional attributes or several vectors added together.\nA fundamental aspect of good computing is understanding how R saves the information.\nWe can also concatenate vectors of different types. However, atomic vectors cannot contain more than one type of data, so it coerces values into a easier way.\nWe are starting to think about these vector / dataframes. A vector is a one dimensional, matrix is two dimensions, arrays are three dimensional.\n\nExample below for demonstrating different aspects of R such as packages, objects, functions, etc. and for improving code readability.\n\nThe assignment arrow is pronounced “gets”, so the name gets the thing on the right.\n\n\n# poor formatting makes cord hard to read \nlibrary(tidyverse)\nrealestate&lt;-read.csv(\"Files/Data/realestate.csv\")\nprice_means&lt;-rep(NA   ,1000);price_medians&lt;-rep(NA   ,1000)\nfor(b in 1:1000){`bootstrap sample`&lt;-sample_n(realestate  ,nrow(realestate)  ,T)\nprice_means[b]&lt;-mean(`bootstrap sample`$price   )\nprice_medians[b]&lt;-median(`bootstrap sample`$price    )}\nquantile(price_means   ,c(0.025,0.975));quantile(price_medians   ,c(0.025,0.975));t.test(realestate$price)\n\nReformat R scripts\n\n\nUse the following shortcuts in RStudio to reformat your code in the qmd file:\n\nCmd + I: Fixes line indentations.\nCmd + Shift + A: Does a complete reformat of the selected part of a code.\n\n\n\n\n# use keyboard shortcut for quick fixes\nlibrary(tidyverse)\nrealestate &lt;- read.csv(\"Files/Data/realestate.csv\"); price_means &lt;- rep(NA   , 1000)\nprice_medians &lt;- rep(NA   , 1000)\nfor (b in 1:1000) {\n  `bootstrap sample` &lt;- sample_n(realestate  , nrow(realestate)  , T)\n  price_means[b] &lt;- mean(`bootstrap sample`$price)\n  price_medians[b] &lt;- median(`bootstrap sample`$price)\n}\nquantile(price_means   , c(0.025, 0.975))\nquantile(price_medians   , c(0.025, 0.975))\nt.test(realestate$price)\n\nUse automated code parsing and then copy result into qmd file.\n\nUse knitr chunk options with formatR installed: tidy: formatR. This parses the code with tidy_source() behind the scenes.\nCan specify additional options with #| tidy-opts: #|   -. View available ones in the help documentation ?tidy_source or this r markdown cookbook section.\n\n\n# automated code parsing -&gt; this has the same source as the first ugly code,\n# but echos nicely\nlibrary(tidyverse)\nrealestate &lt;- read.csv(\"Files/Data/realestate.csv\")\nprice_means &lt;- rep(NA, 1000)\nprice_medians &lt;- rep(NA, 1000)\nfor (b in 1:1000) {\n    `bootstrap sample` &lt;- sample_n(realestate, nrow(realestate), T)\n    price_means[b] &lt;- mean(`bootstrap sample`$price)\n    price_medians[b] &lt;- median(`bootstrap sample`$price)\n}\nquantile(price_means, c(0.025, 0.975))\nquantile(price_medians, c(0.025, 0.975))\nt.test(realestate$price)\n\nRebuilding code for readability (with comments)\n\n# now we can add comments and space out the code for better readability\nlibrary(tidyverse)\nrealestate &lt;- read.csv(\"Files/Data/realestate.csv\")\n\n# initializing the storage space for the bootstrap means and medians\nprice_means &lt;- rep(NA, 1000)\nprice_medians &lt;- rep(NA, 1000)\n\n# loop over 1000 repeated bootstrap samples, saving the means and medians\nfor (b in 1:1000) {\n  bootstrap_sample &lt;- sample_n(realestate, nrow(realestate), T)\n  price_means[b] &lt;- mean(bootstrap_sample$price)\n  price_medians[b] &lt;- median(bootstrap_sample$price)\n}\n\n# calculate the quantiles of the collected means and medians\nquantile(price_means, c(0.025, 0.975))\n\n    2.5%    97.5% \n265932.7 290550.6 \n\nquantile(price_medians, c(0.025, 0.975))\n\n  2.5%  97.5% \n219450 240000 \n\n# do these intervals match will with confidence intervals using central limit theorem?\n# -&gt; compare interval to t-interval\nt.test(realestate$price)\n\n\n    One Sample t-test\n\ndata:  realestate$price\nt = 46.034, df = 521, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 266034.8 289753.5\nsample estimates:\nmean of x \n 277894.1",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Beyond Dataframes</span>"
    ]
  },
  {
    "objectID": "beyond-dataframes.html#advanced-programming",
    "href": "beyond-dataframes.html#advanced-programming",
    "title": "1  Beyond Dataframes",
    "section": "\n1.4 Advanced Programming",
    "text": "1.4 Advanced Programming\n\n``We are adventurers! We do not play on the playgrounds. We roam in the jungles!’’ — Jie Li (Dr. Maurer’s Grad Classmate)\n\nAs we move through this mini-course, we will focus on challenging our programming skills with methods that require heavier statistical computation than typically used in our other courses. For this, we may need to use some more advanced types of objects in R, to use functions in more sophisticated ways and to build our own functions. This section will focus on more advanced data objects than vectors and data frames.\nWe will focus on two particularly helpful data structures:\n\nArrays\nLists\n\n\n1.4.1 Arrays\nVectors don’t have attributes, but we can add attributes that can change how the data is organized.\nAn array in R, is a vector for a \\(p\\)-dimensional index set. This dimension attribute allows the vector to be searched and subsetted very efficiently. Technically we have already seen a 2-dimensional vector; an R matrix. Because an array is a special kind of vector, all values stored inside an array must be the same atomic type.\nWhen doing computation things, if we are only guessing how R handles everything, we cannot leverage how R works to have more efficient code. Note that str() is structure, not string.\n\n# show that a matrix is a vector with dimension attributes\nsome_values &lt;- 1:20\nsome_values\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nstr(some_values)\n\n int [1:20] 1 2 3 4 5 6 7 8 9 10 ...\n\nattributes(some_values)\n\nNULL\n\nis.vector(some_values)\n\n[1] TRUE\n\nis.matrix(some_values)\n\n[1] FALSE\n\nis.array(some_values)\n\n[1] FALSE\n\n# add attributes, turning this into a matrix\ndim(some_values) &lt;- c(4,5)\nattributes(some_values)\n\n$dim\n[1] 4 5\n\nis.vector(some_values)\n\n[1] FALSE\n\nis.matrix(some_values)\n\n[1] TRUE\n\nis.array(some_values)\n\n[1] TRUE\n\n# we can define matrix with the matrix() function\nmy_matrix &lt;- matrix(some_values, nrow = 4, byrow = TRUE)\nis.vector(my_matrix)\n\n[1] FALSE\n\nis.matrix(my_matrix)\n\n[1] TRUE\n\nis.array(my_matrix)\n\n[1] TRUE\n\n\nDemonstrating Higher Dimensional Arrays:\n\nWhen unpacking a matrix (i.e. converting it back to a vector), R goes down the columns rather than across the rows.\n\n\n# unpack matrix\nmy_matrix\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n\nas.vector(my_matrix) \n\n [1]  1  6 11 16  2  7 12 17  3  8 13 18  4  9 14 19  5 10 15 20\n\n\n\nAn array allows us to add an indexing structure (so we can subset these with square brackets), also allows sorting.\n\n\n# build an array here\nvec_to_array &lt;- 1:30\n\n# to convert the vector into an array: R went down the columns, across the rows, and then onto different layers\ndim(vec_to_array) &lt;- c(3,5,2)\nvec_to_array\n\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    4    7   10   13\n[2,]    2    5    8   11   14\n[3,]    3    6    9   12   15\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   16   19   22   25   28\n[2,]   17   20   23   26   29\n[3,]   18   21   24   27   30\n\n# we can pull off a value by using bracket indexing\nvec_to_array[1,3,2]\n\n[1] 22\n\n\nThis can be very useful when the layers have characteristics (e.g. for simulations).\nWhy is this helpful? Suppose we are collecting the resulting values for simulating \\(n = 100\\) observations, from normals with means in \\(\\{-3,0,3\\}\\) and standard deviations in \\(\\{2,10\\}\\)\nUse an array to store the observation index in the first dimension, the means in the second dimension, and standard deviations in the third dimension. Thus we create a 100 by 3 by 2 array to store the data. This allows us to easily access the data we want using the indexing.\n\n# initialize simulation settings\nn &lt;- 100\nmeans &lt;- c(-10,0,10)\nsds &lt;- c(2,10)\n\n# initialize an empty array\nsim_data_array &lt;- array(rep(NA, n *length(means) * length(sds)),\n                        dim = c(n, length(means), length(sds)))\n\n# save the 100 simulated values into the array with corresponding mean and sd positions\nset.seed(12345)\nfor (j in 1:length(means)){\n  for (k in 1:length(sds)){\n    sim_data_array[ ,j, k] &lt;- rnorm(n, mean = means[j], sd = sds[k])\n  }\n}\n\nhead(sim_data_array)\n\n, , 1\n\n           [,1]       [,2]      [,3]\n[1,]  -8.828942 -2.8722914 11.255930\n[2,]  -8.581068 -1.2585193 10.004288\n[3,] -10.218607  0.4870435 10.568755\n[4,] -10.906994  2.1167245  7.996442\n[5,]  -8.788225  1.6626976  8.765556\n[6,] -13.635912  0.2104236 11.656388\n\n, , 2\n\n           [,1]       [,2]        [,3]\n[1,]  -7.760746  5.2228217  -4.2032386\n[2,] -21.562233  0.0979376 -14.6693865\n[3,]  -5.775815 -4.4052620  14.8471584\n[4,] -23.247553 11.9948953   0.6202769\n[5,]  -8.589157 -1.1746849  43.3073331\n[6,] -15.360480  0.3820979   8.3705454\n\n# explore values from second mean and second sd N(0,10)\nsummary(sim_data_array[,2,2])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-25.819  -3.191   3.466   2.153   6.968  22.679 \n\n\nWe can set up an array that matches our simulation structure so that we can compactly store the results to the right locations. Results can be organized like subscripts in mathematical notation. For example, if rows \\(i = 1, \\ldots, 100\\), columns \\(\\mu = (mu_1, \\mu_2, \\mu_3)\\) and layers \\(\\sigma = (\\sigma_1, \\sigma_2)\\), we can have observation \\(x_{i,j,k}\\) that is the\\(i\\)th iteration from mean \\(\\mu_j\\) and standard deviation \\(\\sigma_k\\). This aligns what we are computing (/ how the results are stored) to how we assign the parameter.\nArrays like this are very efficient (because the index structure is very fast in terms of extracting the numbers). Probably wouldn’t want to go more than 3 (maybe 4?) dimensions. Lists are then more readable (less abstract that 10-dimensional hypercube), user friendly ways to store high dimensional data, which can also be indexed easily.\n\n1.4.2 Lists\nA list in R is an indexed set of other R objects that can be named. These objects do not all need to be the same type. Lists can help us to bundle together vectors, matrices, data frames, arrays, or even other lists.\nThese are great for storing results that are contextually related to each other. Examples include results from lm() objects.\n\n# build a list with several different object types\nmy_list &lt;- list(\n  num_vec = c(1,2,3,4,5),\n  letters = LETTERS[1:10],\n  hi = \"Hi Mom!\",\n  my_df = data.frame(x=1:5,y=rnorm(5)),\n  my_array = array(data=1:12,dim=c(3,2,2))\n)\nstr(my_list)\n\nList of 5\n $ num_vec : num [1:5] 1 2 3 4 5\n $ letters : chr [1:10] \"A\" \"B\" \"C\" \"D\" ...\n $ hi      : chr \"Hi Mom!\"\n $ my_df   :'data.frame':   5 obs. of  2 variables:\n  ..$ x: int [1:5] 1 2 3 4 5\n  ..$ y: num [1:5] -1.637 0.212 -0.465 -0.662 -0.133\n $ my_array: int [1:3, 1:2, 1:2] 1 2 3 4 5 6 7 8 9 10 ...\n\n# demonstrate indexing\nmy_list[[3]]\n\n[1] \"Hi Mom!\"\n\nmy_list[[1]] * 2\n\n[1]  2  4  6  8 10\n\nmy_list[[4]][ ,2]\n\n[1] -1.6366291  0.2115626 -0.4648317 -0.6623572 -0.1329536\n\n# demonstrate nested naming\nmy_list$hi\n\n[1] \"Hi Mom!\"\n\nmy_list$my_df$y\n\n[1] -1.6366291  0.2115626 -0.4648317 -0.6623572 -0.1329536\n\n\nAccessing items in the list naming convention:\n\nFor the top layer qualitative information, use list$name notation, then switch to list$name[[index]] for quantitative information that is maybe performed iteritively, like an index.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Beyond Dataframes</span>"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "This section contains the assignments from the course.",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "bootstrap.html#the-bootstrap",
    "href": "bootstrap.html#the-bootstrap",
    "title": "4  Bootstrap",
    "section": "\n4.1 The Bootstrap",
    "text": "4.1 The Bootstrap\nThe statistical procedure known at the bootstrap was born out of Brad Efron’s work in 1979. The bootstrap is named after the hyperbolic phrase “pulling yourself up by your bootstraps” – a physical act that would be impossible – because the bootstrap seems to work in a somewhat unbelievable manner for accomplishing statistical inference.\nWe know that most of the inferential statistics that we have seen previously have been based on knowing theoretical properties of sampling distributions, then working to align our construction of confidence intervals or test-statistics with these known distributions (example knowing CLT and using that for confidence intervals). This is why many of our tests focus around the used of means and proportions, because they have well known properties in large sample sizes. But how do we do inference if the behavior of the sampling distribution isn’t already known? Bootstrapping is one possible solution.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#bootstrap-sampling",
    "href": "bootstrap.html#bootstrap-sampling",
    "title": "4  Bootstrap",
    "section": "\n4.2 Bootstrap Sampling",
    "text": "4.2 Bootstrap Sampling\nNormally we would repeatedly sample from the population and compute our summary statistic of interest. This would tell us what values we can expect from repeated random samples and how uncertain we should be in any particular sample we have. Formally, a sampling distribution for a sample statistic is the probability structure for outcomes of the statistic in repeated sampling from the population. If we don’t have theoretical properties that tell us how that will behave, our solution is simple… just sample repeatedly from the population to find out the distribution of sample statistics. Wait. That is not very practical, since we used our available resources to collect our one real dataset.\nIn bootstrapping, we exchange the idea of resampling from our population, and instead resample from our sample with replacement! In essence, we take the sample that we have and treat it as the entire population (more specifically because our sample is probably small to be a population, we treat many copies of our sample as the population). Then by repeatedly sampling with replacement (each time we will get a different subsample), we can get the randomness from sample to sample that the sampling distribution represents (takes account of). If the sample is representative, then the population should behave like many copies of my sample.\nTerminology: A bootstrap sample is a random resampling of \\(n\\) from the \\(n\\) observations in our original sample, with replacement. The summary statistic that you calculate from a bootstrap sample, is the bootstrap statistic. The distribution of bootstrap statistics that you obtain by repeatedly generating bootstrap samples is called the bootstrap distribution.\nThe amazing thing about the bootstrap distribution is that the shape and spread of the bootstrap distribution are surprisingly good estimate for the shape and spread of the sampling distribution for the same summary statistic. The primary difference is that instead of sampling distribution having an expected value equal to the population parameter, the bootstrap distribution has an expected value equal to the original sample statistic.\nSo suppose you have a summary statistic that doesn’t have a known sampling distribution. How can you do inference? Generate a bunch of bootstrap samples, use these to compute the bootstrap statistics, then use the variability estimates from the bootstrap distribution as approximations to help us build intervals or p-values.\nBootstrap is centered at the sample statistic, rather than the true population statistic (so it is a bit off); but the variability (i.e. spread) of the bootstrap distribution is a very good stand-in for the variation in the sampling distribution. The shape of the bootstrap distribution is also well mimicked. And the larger original sample that we have / more representative of the population that it is, the better the estimates are of the true variability. So we assume that the location of our intervals and test statistics come from the sample, but we want to get the variability information (e.g. standard errors) from our bootstrap distribution. We can do this for lots of different scenarios that we would have no idea what the theoretical properties are, but with bootstrapping we can still do inference. It makes inference much more flexible and computation focused (rather than theoretical) and we can estimate distribution forms that were not previously possible, we just need to be able to resample.\nIn short, bootstrapping generates a sampling distribution from the sample. The variability is from the resampling (each bootstrap sample sometimes hits subjects more often). So there is a point estimate from the real data for the summary statistic (like in any other setting), say the 80th percentile, then construct bootstrap confidence intervals using variability estimates from the bootstrap distribution.\nCompared to the traditional frequentist CI, bootstrapping is a different method for estimating the same thing (so it is a different type interval). There is lots of theory underpinning bootstrapping and when it works well / doesn’t work, but there is no general rule for when bootstrapping would be better than frequentist / bayesian methods. However, bootstrapping is surprisingly good in a wide variety of settings.\nAlthough it still cannot really handle small samples. Yes it doesn’t need an assumption of large sample for CLT, but it is unlikely that a small sample of say 10 observations will be representative of any population, variability will be high. But we can still perform the inferences.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#examples",
    "href": "bootstrap.html#examples",
    "title": "4  Bootstrap",
    "section": "\n4.3 Examples",
    "text": "4.3 Examples\n\n4.3.1 Simple example\nHere is a simple example to show how to get a bootstrap sample.\n\n# lets explore what a bootstrap sample looks like\n\n# original sample\nsimple_data &lt;- data.frame(row = 1:5, value = c(7,6,9,12,11))\nsimple_data\n\n  row value\n1   1     7\n2   2     6\n3   3     9\n4   4    12\n5   5    11\n\n# single bootstrap sample\nsimple_data[sample(1:5, replace = TRUE), ]\n\n    row value\n3     3     9\n5     5    11\n1     1     7\n3.1   3     9\n5.1   5    11\n\n# row names are indexed by the number of additional times they are included\n# -&gt; so 3.1 means the second time the third obs was included in the sample\n\nNote that only 5 obs would obviously not be a good representative of any important population of interest. So lets work with a bigger dataset and a complex statistic.\n\n4.3.2 Principal compenent bootstrap\nOlive oil dataset contains 8 numeric columns of different acids and suppose we want to do some kind of dimension reduction technique, specifically principal component analysis (PCA).\n\nlibrary(tidyverse)\nlibrary(pdfCluster)\ndata(\"oliveoil\")\nhead(oliveoil)\n\n  macro.area       region palmitic palmitoleic stearic oleic linoleic linolenic\n1      South Apulia.north     1075          75     226  7823      672        36\n2      South Apulia.north     1088          73     224  7709      781        31\n3      South Apulia.north      911          54     246  8113      549        31\n4      South Apulia.north      966          57     240  7952      619        50\n5      South Apulia.north     1051          67     259  7771      672        50\n6      South Apulia.north      911          49     268  7924      678        51\n  arachidic eicosenoic\n1        60         29\n2        61         29\n3        63         29\n4        78         35\n5        80         46\n6        70         44\n\n\nPCA idea:\n\nInstead of 8 dimensions explaining the differences / relationships between olive oils, perhaps only 2 or 3 dimensions could explain just as much information.\nOne feature of PCA is that is uses the variance-covariance matrix, which is unit-dependent. So currently if we attempt to rotate this space, oleic (whose scale is much bigger) will highlight more than the others. Instead we can use the correlation matrix, which is unit-free, to remedy this. Thus when figuring out the relationships between variables, it uses these unitless z-scores instead it will treat all 8 as equally important.\nprincomp() performs the PCA. It also gives us standard deviations, which we can turn into variance by squaring. These are then very interpretable. By using the correlation matrix, the sum of the variances equals the number of original dimensions. And the ratio of the variance to the original dimension is the proportion of variability explained by each respective principal component.\n\n\n# perform PCA\n# -&gt; x os the matrix we are trying to reduce and we want to use the correlation matrix\noo_pca &lt;- princomp(x = oliveoil[3:10], cor = TRUE)\n\n# check sum of variances and calculate proportion of the variances explained for each principal component\nsum((oo_pca$sdev)^2)\n\n[1] 8\n\nround((oo_pca$sdev)^2 / sum((oo_pca$sdev)^2), 3)\n\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 \n 0.465  0.221  0.127  0.099  0.042  0.031  0.015  0.000 \n\n\nFor example, the interpretation of this summary statistic is that 46.5 % of the variability found in 8 dimensions can be explained by the first principal component. And 68.6 % can be explained by the first two and so on.\nThe first principle component variability percent is what we are going to bootstrap. We can’t be super certain that our one particular value is the correct value, but it is definitely an interpretable summary that we can study.\nGoal: collect the bootstrap statistics from 10,000 bootstrap samples for that proportion of variability explained.\n\n# initialize items\nB &lt;- 10000\nboot_prop_ex &lt;- rep(NA, B)\n\n# loop\nfor (b in 1:B) {\n  \n  # create bootstrap sample\n  boot_samp &lt;- oliveoil[sample(1:nrow(oliveoil), replace = TRUE), ]\n  \n  # calculate PCA\n  oo_pca &lt;- princomp(x = boot_samp[3:10], cor = TRUE)\n  \n  # save explained variability of first principal component\n  boot_prop_ex[b] &lt;- ((oo_pca$sdev)^2 / sum((oo_pca$sdev)^2))[1]\n  \n}\n\n# view results\nhist(boot_prop_ex)\n\n\n\n\n\n\n\n\nsummary(boot_prop_ex)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4294  0.4592  0.4657  0.4660  0.4728  0.5057 \n\n\n\n4.3.3 Bootstrap inference\nThen we can get a central bootstrap interval by getting the lower and upper quantiles of the middle, say 90%, of bootstrap statistics. Replace bootstrap distribution with sampling distribution and we have a confidence interval (they are analogous). This bootstrap interval should have the same width as a regular confidence interval built from resampling the population and the sampling distribution, just centered at the sample statistic instead of the true mean.\n\n# central boootstrap interval\nquantile(boot_prop_ex, c(0.05, 0.5, 0.95))\n\n       5%       50%       95% \n0.4492671 0.4656812 0.4828598 \n\n\nIf wanted to do official hypothesis test, this is the logic (called inverting the test, generate from the sample instead of the sampling distribution):\n\n\n\n\n\n\n\nSuppose the bootstrap distribution is centered at the sample statistic \\(S\\). The shape and spread are exactly like what the true sampling distribution would be.\nNow suppose there is a hypothesized value, say \\(S_0 = 0.5\\). This is where the sampling distribution should be centered. We know the variability we should see around that value.\nSo \\(S_0\\) being in the tail of the bootstrap distribution is like \\(S\\) being in the tail of the sampling distribution. We are inverting the roles of the test statistic and the center point. We can use this to do testing.\nBeing way out in the tails is evidence that the hypothesized value probably isn’t the true value; same logic as regular hypothesis test. Formally, we could actually shift all the points in the bootstrap dist and calculate a p-value.\n\n\n# check other quantities\nsum(boot_prop_ex &gt;= 0.5)\n\n[1] 9\n\nmean(boot_prop_ex &gt;= 0.5)\n\n[1] 0.0009\n\n# shift bootstrap distribution to S_not\n# -&gt; calculate sample statistic found earlier (in one line!)\nS &lt;- (princomp(x = oliveoil[3:10], cor = TRUE)$sdev)^2 %&gt;% magrittr::divide_by(sum(.)) %&gt;% .[1]\nS_not &lt;- 0.5\nboot_prop_ex_inverted &lt;- boot_prop_ex + (abs(mean(boot_prop_ex) - S_not))\n\n# calculate p-value\nmean(boot_prop_ex_inverted &lt;= S)\n\n[1] 0.0001\n\n# visualize shift\nggplot() +\n  geom_density(aes(x = boot_prop_ex),\n               color = \"lightblue\",\n               fill = \"lightblue\",\n               alpha = 0.20) + \n  geom_density(aes(x = boot_prop_ex_inverted),\n               color = \"purple\",\n               fill = \"purple\",\n               alpha = 0.20) + \n  geom_vline(aes(xintercept = S),\n             color = \"lightblue\") + \n    geom_vline(aes(xintercept = S_not),\n             color = \"purple\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n4.3.4 F-test bootstrap\nUse bootstrapping to reproduce the following ANOVA F-test for the difference in overall mean Palmitic Acid for olive oils in different macro areas.\n\n# find alternative bootstrap to a common test\nmod &lt;- lm(stearic ~ macro.area, data = oliveoil)\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: stearic\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nmacro.area   2   1273   636.7  0.4707 0.6248\nResiduals  569 769685  1352.7               \n\n# loop to collect the boostrap statistics of the F stat from 10000 bootstrap samples\nboot_anova &lt;- sapply(X = 1:B, function(X) {\n  \n  # generate bootstrap sample\n  # -&gt; fit model\n  # -&gt; anova table\n  # -&gt; extract F-stat\n  oliveoil[sample(1:nrow(oliveoil), replace = TRUE), ] %&gt;% \n    {lm(stearic ~ macro.area, data = .)} %&gt;% \n    anova %&gt;% \n    .[1, 4]\n\n})\n\n# view results\nhist(boot_anova)\n\n\n\n\n\n\n\n\n# compare our p-value to the original by finding prob F &gt; original sample F\nmean(boot_anova &gt;= lm(stearic ~ macro.area, data = oliveoil) %&gt;% anova %&gt;% .[1,4])\n\n[1] 0.7225\n\n# same conclusion as the original F-test\n\nThis also demonstrates that we won’t always get symmetric bootstrap distributions. It depends on the statistic we are studying, the skewness in the population, the sample size, etc. (the same factors as usual for sampling distributions).\nAdditionally, we have been doing thus far is the simple bootstrap. We can try other sampling techniques as well. Another strategy would be to do a stratified sample rather than sampling with replacement for the whole sample. This would entail bootstrapping within each group, so the group sample sizes of macro.area are the same in our bootstrap samples as in the original sample.\n\n# collect a single stratified bootstrap sample\n# -&gt; split by macro.area, bootstrap within each, then combine results back together\nsample_stratified &lt;- oliveoil %&gt;% \n  split(.$macro.area) %&gt;% \n  map(\\(df) df[sample(1:nrow(df), replace = TRUE), ]) %&gt;% \n  reduce(bind_rows)\n\n# confirm group counts\ncount(sample_stratified, macro.area) == count(oliveoil, macro.area)\n\n     macro.area    n\n[1,]       TRUE TRUE\n[2,]       TRUE TRUE\n[3,]       TRUE TRUE\n\n# loop to collect the boostrap statistics of the F stat from 10000 bootstrap samples\nB &lt;- 10000\nboot_anova_stratified &lt;- sapply(X = 1:B, function(X) {\n  \n  # generate stratified bootstrap sample\n  # -&gt; fit model\n  # -&gt; anova table\n  # -&gt; extract F-stat\n  oliveoil %&gt;% \n  split(.$macro.area) %&gt;% \n  map(\\(df) df[sample(1:nrow(df), replace = TRUE), ]) %&gt;% \n  reduce(bind_rows) %&gt;% \n    .[sample(1:nrow(oliveoil), replace = TRUE), ] %&gt;% \n    {lm(stearic ~ macro.area, data = .)} %&gt;% \n    anova %&gt;% \n    .[1, 4]\n\n})\n\n# view results\nhist(boot_anova_stratified)\n\n\n\n\n\n\n\n\n# compare our p-value to the original by finding prob F &gt; original sample F\nmean(boot_anova_stratified &gt;= lm(stearic ~ macro.area, data = oliveoil) %&gt;% anova %&gt;% .[1,4])\n\n[1] 0.8012\n\n# same conclusion as the simple random bootstrap F-test (even less evidence)",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "computational-experiments.html#overview",
    "href": "computational-experiments.html#overview",
    "title": "5  Computational Experiments",
    "section": "\n5.1 Overview",
    "text": "5.1 Overview\nIn the process of running analyses and working with new methods, we often encounter the situation where we don’t know what the best course of action is for continuing our work. It may not be clear if our new method is really any more accurate than existing methods, or which algorithm is more efficient, or which parameterization is best. If we look to existing literature, web resources and professional advice, we might find a solution already exists; or we might learn that there is not clear answer, and we have to figure it out ourselves. What do we do then?\nExample: In a standard knn regression, we predict the response value for a new point by finding the k-nearest points from the training data, then calculating their average as a prediction for the new point. This is a nice non-parametric model but it requires a lot of distance calculation to find the neighbors. Suppose that a researcher is working on a new approximation to a k-nearest neighbor regression model. Instead of creating neighborhoods based on distance, instead the neighborhoods are defined by binning the features using quantile-based bins. The researcher thinks that this should be faster than knn, but is unsure if the predictions will be as accurate.\nExample: You have heard of stepwise variable selection for regression. Suppose you also learn the LASSO regression can also be used to run variable selection. Which will lead to smaller models? Which is faster? Which makes a model that is better for predictive accuracy?\n\n# load packages\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(caret)",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computational Experiments</span>"
    ]
  },
  {
    "objectID": "computational-experiments.html#sec-methods-preview",
    "href": "computational-experiments.html#sec-methods-preview",
    "title": "5  Computational Experiments",
    "section": "\n5.2 Methods preview",
    "text": "5.2 Methods preview\nWe are going to build a regression model for prediction. We can start with using a single dataset, air quality. So we are going to predict air quality based on certain chemicals and we want to see which combinations of variables leads to the best predictions.\n\n# load and preview data\nload(\"files/data/air_quality_cleaned.Rdata\")\nglimpse(data)\n\nRows: 9,357\nColumns: 13\n$ y            &lt;dbl&gt; 0.4739746, 0.4662483, 0.4688237, 0.4688237, 0.4610975, 0.…\n$ PT08S1_CO_   &lt;dbl&gt; 0.942932371, 0.736767251, 1.070269650, 0.991441811, 0.676…\n$ NMHC_GT_     &lt;dbl&gt; 2.211117, 1.939279, 1.767592, 1.710363, 1.502908, 1.40991…\n$ C6H6_GT_     &lt;dbl&gt; 0.242490732, 0.182075374, 0.172408916, 0.177242145, 0.111…\n$ PT08S2_NMHC_ &lt;dbl&gt; 0.44227291, 0.17645006, 0.12971198, 0.15600215, -0.171164…\n$ NOx_GT_      &lt;dbl&gt; -0.01016561, -0.25488865, -0.14612285, 0.01314135, -0.146…\n$ PT08S3_NOx_  &lt;dbl&gt; 0.81060577, 1.17707274, 1.07148056, 0.92240926, 1.2733479…\n$ NO2_GT_      &lt;dbl&gt; 0.43210124, 0.26666934, 0.43997894, 0.50300062, 0.4557343…\n$ PT08S4_NO2_  &lt;dbl&gt; 0.643223131, 0.358554642, 0.349993184, 0.412063757, 0.210…\n$ PT08S5_O3_   &lt;dbl&gt; 0.641066950, -0.006723079, 0.216501863, 0.498815761, 0.29…\n$ T            &lt;dbl&gt; 0.0884577431, 0.0815138810, 0.0491091913, 0.0282776050, 0…\n$ RH           &lt;dbl&gt; 0.1838213, 0.1603912, 0.2833993, 0.4005499, 0.3927398, 0.…\n$ AH           &lt;dbl&gt; 0.1948705, 0.1940418, 0.1946755, 0.1956120, 0.1956659, 0.…\n\n\nTo start we can fit all the variables.\n\n# fit full model\nbig_mod &lt;- lm(y ~ . , data = data)\nsummary(big_mod)\n\n\nCall:\nlm(formula = y ~ ., data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8408 -0.1165  0.1332  0.3043  2.3941 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.734e-17  7.505e-03   0.000  1.00000    \nPT08S1_CO_   -2.355e-02  3.494e-02  -0.674  0.50031    \nNMHC_GT_      8.878e-02  9.380e-03   9.465  &lt; 2e-16 ***\nC6H6_GT_     -2.407e-01  2.728e-01  -0.883  0.37751    \nPT08S2_NMHC_ -5.690e-02  7.628e-02  -0.746  0.45575    \nNOx_GT_       1.206e-01  2.025e-02   5.959 2.62e-09 ***\nPT08S3_NOx_   4.952e-02  2.154e-02   2.299  0.02150 *  \nNO2_GT_       6.300e-01  1.620e-02  38.882  &lt; 2e-16 ***\nPT08S4_NO2_  -1.460e-02  3.499e-02  -0.417  0.67646    \nPT08S5_O3_   -5.765e-02  2.551e-02  -2.260  0.02382 *  \nT             2.587e-01  1.049e-01   2.467  0.01365 *  \nRH            1.572e-01  4.909e-02   3.203  0.00136 ** \nAH           -9.846e-02  2.899e-01  -0.340  0.73413    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.726 on 9344 degrees of freedom\nMultiple R-squared:  0.4737,    Adjusted R-squared:  0.473 \nF-statistic: 700.7 on 12 and 9344 DF,  p-value: &lt; 2.2e-16\n\n\nBut for prediction, this can lead to overfitting. So we need to select variables, specifically which ones are helpful in predicting air quality. One option is to do stepwise selection based on AIC. We see that a few variables are dropped based on AIC, which measures some complexity penalty, and the stepwise procedure is trying to give us a more parsimonious model. The model could then be the final model used for prediction.\n\n# stepwise variable selection based on AIC\n# -&gt; direction = \"both\" by default\nstep_selected &lt;- step(big_mod)\n\nStart:  AIC=-5980.41\ny ~ PT08S1_CO_ + NMHC_GT_ + C6H6_GT_ + PT08S2_NMHC_ + NOx_GT_ + \n    PT08S3_NOx_ + NO2_GT_ + PT08S4_NO2_ + PT08S5_O3_ + T + RH + \n    AH\n\n               Df Sum of Sq    RSS     AIC\n- AH            1      0.06 4924.5 -5982.3\n- PT08S4_NO2_   1      0.09 4924.5 -5982.2\n- PT08S1_CO_    1      0.24 4924.7 -5982.0\n- PT08S2_NMHC_  1      0.29 4924.7 -5981.9\n- C6H6_GT_      1      0.41 4924.8 -5981.6\n&lt;none&gt;                      4924.4 -5980.4\n- PT08S5_O3_    1      2.69 4927.1 -5977.3\n- PT08S3_NOx_   1      2.79 4927.2 -5977.1\n- T             1      3.21 4927.6 -5976.3\n- RH            1      5.41 4929.8 -5972.1\n- NOx_GT_       1     18.72 4943.1 -5946.9\n- NMHC_GT_      1     47.21 4971.6 -5893.1\n- NO2_GT_       1    796.74 5721.2 -4579.2\n\nStep:  AIC=-5982.3\ny ~ PT08S1_CO_ + NMHC_GT_ + C6H6_GT_ + PT08S2_NMHC_ + NOx_GT_ + \n    PT08S3_NOx_ + NO2_GT_ + PT08S4_NO2_ + PT08S5_O3_ + T + RH\n\n               Df Sum of Sq    RSS     AIC\n- PT08S4_NO2_   1      0.04 4924.5 -5984.2\n- PT08S1_CO_    1      0.23 4924.7 -5983.9\n- PT08S2_NMHC_  1      0.24 4924.7 -5983.8\n&lt;none&gt;                      4924.5 -5982.3\n- PT08S5_O3_    1      2.80 4927.3 -5979.0\n- PT08S3_NOx_   1      2.91 4927.4 -5978.8\n- C6H6_GT_      1      3.03 4927.5 -5978.5\n- T             1      4.99 4929.5 -5974.8\n- RH            1      7.16 4931.6 -5970.7\n- NOx_GT_       1     21.65 4946.1 -5943.3\n- NMHC_GT_      1     48.36 4972.8 -5892.9\n- NO2_GT_       1    879.80 5804.3 -4446.2\n\nStep:  AIC=-5984.21\ny ~ PT08S1_CO_ + NMHC_GT_ + C6H6_GT_ + PT08S2_NMHC_ + NOx_GT_ + \n    PT08S3_NOx_ + NO2_GT_ + PT08S5_O3_ + T + RH\n\n               Df Sum of Sq    RSS     AIC\n- PT08S1_CO_    1      0.26 4924.8 -5985.7\n&lt;none&gt;                      4924.5 -5984.2\n- PT08S2_NMHC_  1      1.26 4925.8 -5983.8\n- PT08S5_O3_    1      2.78 4927.3 -5980.9\n- PT08S3_NOx_   1      3.19 4927.7 -5980.1\n- C6H6_GT_      1      5.83 4930.4 -5975.1\n- T             1      9.40 4933.9 -5968.4\n- RH            1     11.12 4935.7 -5965.1\n- NOx_GT_       1     23.65 4948.2 -5941.4\n- NMHC_GT_      1     49.42 4974.0 -5892.8\n- NO2_GT_       1    891.42 5816.0 -4429.4\n\nStep:  AIC=-5985.71\ny ~ NMHC_GT_ + C6H6_GT_ + PT08S2_NMHC_ + NOx_GT_ + PT08S3_NOx_ + \n    NO2_GT_ + PT08S5_O3_ + T + RH\n\n               Df Sum of Sq    RSS     AIC\n&lt;none&gt;                      4924.8 -5985.7\n- PT08S2_NMHC_  1      1.82 4926.6 -5984.3\n- PT08S3_NOx_   1      3.61 4928.4 -5980.9\n- PT08S5_O3_    1      3.95 4928.7 -5980.2\n- C6H6_GT_      1      6.19 4931.0 -5976.0\n- T             1      9.38 4934.2 -5969.9\n- RH            1     10.91 4935.7 -5967.0\n- NOx_GT_       1     23.50 4948.3 -5943.2\n- NMHC_GT_      1     54.60 4979.4 -5884.5\n- NO2_GT_       1    891.52 5816.3 -4430.9\n\nstep_selected\n\n\nCall:\nlm(formula = y ~ NMHC_GT_ + C6H6_GT_ + PT08S2_NMHC_ + NOx_GT_ + \n    PT08S3_NOx_ + NO2_GT_ + PT08S5_O3_ + T + RH, data = data)\n\nCoefficients:\n (Intercept)      NMHC_GT_      C6H6_GT_  PT08S2_NMHC_       NOx_GT_  \n   4.900e-17     8.526e-02    -2.994e-01    -6.404e-02     1.238e-01  \n PT08S3_NOx_       NO2_GT_    PT08S5_O3_             T            RH  \n   5.002e-02     6.277e-01    -6.467e-02     2.179e-01     1.353e-01  \n\n\nOr we could use another method such as LASSO variable selection. LASSO is a fairly common method that selects variables based on a different complexity penalty (not AIC), specifically it is based on shrinkage penalty (controlled by \\(\\lambda\\) parameter). In a standard regression setting, there is a way of solving the \\(\\boldsymbol{\\beta}\\) equation to minimize the SSE (or MSE).\nWhen we fit a LASSO model, we add a penalty \\(\\lambda\\) for complexity (i.e. \\(\\beta\\)s that are very big in magnitude, which means they stay included in the model). Essentially, we solve for \\(\\boldsymbol{\\beta}\\) while conditioning them to be generally smaller. And because LASSO can become more and more aggressive if we penalize more and more (i.e. increase \\(\\lambda\\)), LASSO can push some of the \\(\\beta\\)s all the way down to zero. So we might end up with some \\(\\beta_i = 0\\) in a constrained solution. Thus the corresponding \\(X\\) variable is essentially removed from the model when we multiply \\(\\boldsymbol{X} \\boldsymbol{\\beta}\\).\nSo we have to tune the \\(\\lambda\\) to get a value that results in good performance (parameter tuning). We can use cv.glmnet() to cross-validate to pick a \\(\\lambda\\) value that is tuned for optimal performance based on the specified measure. In the example below, we chose deviance which is measure of model fit (commonly used by computer scientists). We could have picked for example MSE or MAE, it is essentially the same type of decision as using AIC vs BIC as a model criteria (or penalty).\n\n# pick variables with LASSO\n# -&gt; pick lambda based on a cross validated tuning process\n# -&gt; x = predictor variables, y = response\ncv.out &lt;- cv.glmnet(x = as.matrix(x = data[ , -which(names(data) == \"y\")]),\n                    y = data$y, alpha = 1, type.measure = \"deviance\")\n\n# view tuned lambda\ncv.out$lambda.1se\n\n[1] 0.04117753\n\n# use tuned lambda to pick variables\nlasso_mod &lt;- glmnet(x = as.matrix(x = data[ , -which(names(data) == \"y\")]),\n                    y = data$y, alpha = 1, lambda = cv.out$lambda.1se)\n\n# view resulting coefficient estimates\nlasso_mod$beta[,1]\n\n  PT08S1_CO_     NMHC_GT_     C6H6_GT_ PT08S2_NMHC_      NOx_GT_  PT08S3_NOx_ \n  0.00000000   0.02408635   0.00000000  -0.04278944   0.00000000   0.02913581 \n     NO2_GT_  PT08S4_NO2_   PT08S5_O3_            T           RH           AH \n  0.64247636   0.00000000   0.00000000   0.00000000   0.00000000   0.00000000 \n\n# find variables whose coefficients were NOT shrunk to zero\nlasso_vars &lt;- names(lasso_mod$beta[,1])[which(lasso_mod$beta[,1] != 0)]\n\n# use selected variables to fit the linear model\n\n# first convert the names of the selected variables to a formula string\nlasso_vars\n\n[1] \"NMHC_GT_\"     \"PT08S2_NMHC_\" \"PT08S3_NOx_\"  \"NO2_GT_\"     \n\npaste(lasso_vars, collapse = \" + \")\n\n[1] \"NMHC_GT_ + PT08S2_NMHC_ + PT08S3_NOx_ + NO2_GT_\"\n\npaste0(\"y ~ \",paste(lasso_vars, collapse = \" + \"))\n\n[1] \"y ~ NMHC_GT_ + PT08S2_NMHC_ + PT08S3_NOx_ + NO2_GT_\"\n\n# fit model based on lasso selected variables\nlasso_selected &lt;- lm(formula(paste0(\"y ~ \",paste(lasso_vars, collapse = \" + \"))), data = data )\nlasso_selected\n\n\nCall:\nlm(formula = formula(paste0(\"y ~ \", paste(lasso_vars, collapse = \" + \"))), \n    data = data)\n\nCoefficients:\n (Intercept)      NMHC_GT_  PT08S2_NMHC_   PT08S3_NOx_       NO2_GT_  \n  -2.502e-17     6.240e-02    -9.494e-02     7.983e-02     7.019e-01  \n\n\nThis method suggest cut more variables than the stepwise selection, so it was more aggressive for variable selection. So which of these approaches is superior? Depends on what we are trying to ask. To gather an understanding of how they behave, we should try them out in a comprehensive way (multiple datasets): does lasso tend to pick more variables than AIC or was it just a weird dataset? Maybe it won’t always be this way. So we need to do side by side comparison in a comprehensive, methodical way with an experiment.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computational Experiments</span>"
    ]
  },
  {
    "objectID": "computational-experiments.html#preparing-to-experiment",
    "href": "computational-experiments.html#preparing-to-experiment",
    "title": "5  Computational Experiments",
    "section": "\n5.3 Preparing to experiment",
    "text": "5.3 Preparing to experiment\nIn experimental design, what are key features? Response of interest, treatments, subjects, control, randomization, replication, reproducibility. We need to fit these into the framework of our computational experiment (essentially data collection).\nIn the knn regression example above identify the following:\nComputational experiment\n\n\nWhat are the responses of interest? What do we want to know or be able to measure for the two methods?\n\nPredictive performance (accuracy metric such as RMSE on testing data)\nNumber of variables selected (maybe proportion of total selected: transformation to make it more relative to the dataset size); does one select simpler (more parsimonious) models?\nVariability of the coefficients (does a smaller model end up with more uncertatinty in the \\(\\beta\\)s?)\nTiming comparison (time to select and fit model; comparison of how fast the process is on the same dataset)\n\n\n\nWhat are the treatments that we want to test above? What are we going to systematically change between each trial run of this experiment?\n\nSelect algorithm (lasso vs step)\nParameter sets (need to tune parameters, can change how to tune – lasso: lambda, stepwise: AIC/BIC)\nNOTE for the experiment below: Even though lasso is super flexible and we can penalize more or less, we are going to use some preset criteria for how to set the value (kinda like for stepwise where we are picking a preset criteria of AIC)\n\n\n\nWhat are the subjects? What are we applying the treatment to?\n\nDatasets (9 real datasets from the UCI repository)\nNOTE we are concerned with accuracy, so better to use real data than simulated data\nNOTE in a more classic, experimental setting where we have more control over the subjects, we could simulate the datasets in a sequential way and change certain aspects (e.g. we can change the covariance matrix); this way we know how the treatments should behave because we simulated the subjects in a particular manner (we can design the subjects to match the characteristics we want to test)\n\n\n\nWhat do we need to control? Kinda like blocks in a real study, but what does this mean in a computational experiment?\n\nPredefined structure in code and design that either sets up consistant parameters or processes (this allows for correct comparison of results)\n(e.g. tuning parameters: if the \\(\\lambda\\) is not what is being studied for LASSO, the values can be different from dataset to dataset but they need to be found in a consistent way (a prescribed method))\nRemoving the effect of magnitude for the \\(X\\) variables (i.e. standardizing all \\(X\\) variable information so they are on same scales, this is like a process / setup that the datasets had to go through first to ensure the scales don’t introduce a different treatment)\n\n\n\nWhat roll does randomization play? Randomization of individuals to groups, how does this translate to computational experiments?\n\nWill talk about this later, but want to align simulation approach so we know that we have balance across all other factors\n\n\n\nWhat do we do for replication?\n\nTiming replication; can do bootstrapping on timings because there could be other processes running on computer while doing the timer that affect the timing results, so we want to collect many timings\nBootstrap datasets for variable selection; will LASSO always select 4 variables, what if we permute the data? We want to try to capture some of that random behavior\n\n\n\nHow do I make it all reproducible?\n\nHow to layout code file to know what data cleaning was done ahead of time, what functions are being called throughout the experiment, how the results are organized and doing it in a way that can be replicated.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computational Experiments</span>"
    ]
  },
  {
    "objectID": "computational-experiments.html#organizing-to-support-an-experiment",
    "href": "computational-experiments.html#organizing-to-support-an-experiment",
    "title": "5  Computational Experiments",
    "section": "\n5.4 Organizing to support an experiment",
    "text": "5.4 Organizing to support an experiment\nTo be organized to run a computational experiment, we need many components prepared. Thinking about set of tasks we need to do beforehand will help. There is lots of work to be done before can start getting results.\n\n\nIf we have data that will act as the experimental subject, then we need to have this cleaned and accessible.\n\nIf simulating data, then would want a function to do all the data generation based on the desired properties.\n\n\nIf we have a summary statistic(s) selected as our response of interest, then we need to have functions that can compute and store those values.\nIf we have identified the algorithms and models that will act as our treatments, then we need functions that can implement these methods.\nIf we have a set of possible parameter settings for an algorithm that will act as treatment factors, then we need to have those values organized into a data object that can be accessed when needed.\n\nThis is all to say, there is often a lot of preparation of data and functions that is required before we can start gathering results from computational experiments. We want everything to be organized so that we can automate everything.\n\n5.4.1 Load data\nWe are going to figure out which type of model will behave more accurately in terms of a predictive setting. So working with simulated data sets might not have as much real application.\nWe are going to work with 9 machine learning datasets from real settings (found in the UCI repository). These are commonly used regression data sets for whenever researchers have new methods and they want to see how the models behave on real data (e.g. check their predictive capabilities).\nData prep: All datasets have been put through the following processes\n\nStandardized to remove effect of magnitude\nResponse variable renamed to y so there is no confusion about what the response is (all other \\(X\\) variable names are unchanged)\nSimilar naming structure _cleaned.RData to read in easily\n\nThen we can save them all in organized holding structure for easy access.\n\n# dataset names used for testing\nall_reg_sets &lt;- c(\"wpbc\", \"wankara\", \"laser\", \"treasury\", \"skillcraft\", \"puma\", \"air_quality\", \"ccpp\", \"casp\")\n\n# initialize items\ndata_list &lt;- list(NULL)\nall_reg_sizes &lt;- NA\n\n# loop through dataset names to load all datasets\nfor (i in 1:length(all_reg_sets)){\n  \n  # concatenate path with specific dataset name\n  data_path = paste0(\"files/data/\", all_reg_sets[i], \"_cleaned.Rdata\")\n  \n  # load and assign data\n  load(data_path)\n  data_list[[i]] = data\n  all_reg_sizes[i] = nrow(data)\n  \n  # print loaded variables\n  print(paste(\"vars from data\", all_reg_sets[i]))\n  print(names(data))\n  \n}\n\n[1] \"vars from data wpbc\"\n [1] \"y\"   \"V4\"  \"V5\"  \"V6\"  \"V7\"  \"V8\"  \"V9\"  \"V10\" \"V11\" \"V12\" \"V13\" \"V14\"\n[13] \"V15\" \"V16\" \"V17\" \"V18\" \"V19\" \"V20\" \"V21\" \"V22\" \"V23\" \"V24\" \"V25\" \"V26\"\n[25] \"V27\" \"V28\" \"V29\" \"V30\" \"V31\" \"V32\" \"V33\" \"V34\"\n[1] \"vars from data wankara\"\n [1] \"V1\" \"V2\" \"V3\" \"V4\" \"V5\" \"V6\" \"V7\" \"V8\" \"V9\" \"y\" \n[1] \"vars from data laser\"\n[1] \"V1\" \"V2\" \"V3\" \"V4\" \"y\" \n[1] \"vars from data treasury\"\n [1] \"V1\"  \"V2\"  \"V3\"  \"V4\"  \"V5\"  \"V6\"  \"V7\"  \"V8\"  \"V9\"  \"V10\" \"V11\" \"V12\"\n[13] \"V13\" \"V14\" \"V15\" \"y\"  \n[1] \"vars from data skillcraft\"\n[1] \"LeagueIndex\"      \"y\"                \"UniqueHotkeys\"    \"NumberOfPACs\"    \n[5] \"GapBetweenPACs\"   \"ActionLatency\"    \"ActionsInPAC\"     \"TotalMapExplored\"\n[9] \"UniqueUnitsMade\" \n[1] \"vars from data puma\"\n [1] \"V1\"  \"V2\"  \"V3\"  \"V4\"  \"V5\"  \"V6\"  \"V7\"  \"V8\"  \"V9\"  \"V10\" \"V11\" \"V12\"\n[13] \"V13\" \"V14\" \"V15\" \"V16\" \"V17\" \"V18\" \"V19\" \"V20\" \"V21\" \"V22\" \"V23\" \"V24\"\n[25] \"V25\" \"V26\" \"V27\" \"V28\" \"V29\" \"V30\" \"V31\" \"V32\" \"y\"  \n[1] \"vars from data air_quality\"\n [1] \"y\"            \"PT08S1_CO_\"   \"NMHC_GT_\"     \"C6H6_GT_\"     \"PT08S2_NMHC_\"\n [6] \"NOx_GT_\"      \"PT08S3_NOx_\"  \"NO2_GT_\"      \"PT08S4_NO2_\"  \"PT08S5_O3_\"  \n[11] \"T\"            \"RH\"           \"AH\"          \n[1] \"vars from data ccpp\"\n[1] \"AT\" \"V\"  \"AP\" \"RH\" \"y\" \n[1] \"vars from data casp\"\n [1] \"y\"  \"F1\" \"F2\" \"F3\" \"F4\" \"F5\" \"F6\" \"F7\" \"F8\" \"F9\"\n\n# rename data lists\nnames(data_list) &lt;- all_reg_sets\n\n# view loaded data\nstr(data_list, max.level = 1)\n\nList of 9\n $ wpbc       :'data.frame':    198 obs. of  32 variables:\n $ wankara    :'data.frame':    321 obs. of  10 variables:\n $ laser      :'data.frame':    993 obs. of  5 variables:\n $ treasury   :'data.frame':    1049 obs. of  16 variables:\n $ skillcraft :'data.frame':    3395 obs. of  9 variables:\n $ puma       :'data.frame':    8192 obs. of  33 variables:\n $ air_quality:'data.frame':    9357 obs. of  13 variables:\n $ ccpp       :'data.frame':    9568 obs. of  5 variables:\n $ casp       :'data.frame':    45730 obs. of  10 variables:\n\n\n\n5.4.2 Modularize code\nRather than having one function that does everything, we want to have modularized code where helper functions perform singular tasks (this way they are like action statements).\nThe variable selection helper functions below perform the same steps as in Section 5.2, just generalized for the cleaned data. They should take the subjects as inputs (datasets) and output a model from the treatment.\n\n# helper functions for running the experiment with variable selection\n\n# function for choosing with stepwise and fitting a regression\n# -&gt; inputs dataframe and returns selected model\nstep_var_mod &lt;- function(df){\n  \n  # run stepwise procedure from full model\n  step_selected = step(lm(y ~ . , data = df), trace = FALSE)\n  \n  return(step_selected)\n\n}\n\n# test function on a single dataset\nstep_var_mod(data_list$air_quality)\n\n\nCall:\nlm(formula = y ~ NMHC_GT_ + C6H6_GT_ + PT08S2_NMHC_ + NOx_GT_ + \n    PT08S3_NOx_ + NO2_GT_ + PT08S5_O3_ + T + RH, data = df)\n\nCoefficients:\n (Intercept)      NMHC_GT_      C6H6_GT_  PT08S2_NMHC_       NOx_GT_  \n   4.900e-17     8.526e-02    -2.994e-01    -6.404e-02     1.238e-01  \n PT08S3_NOx_       NO2_GT_    PT08S5_O3_             T            RH  \n   5.002e-02     6.277e-01    -6.467e-02     2.179e-01     1.353e-01  \n\n# function for choosing with lasso and fitting regression\n# -&gt; inputs dataframe and returns selected model\nlasso_var_mod &lt;- function(df){\n  \n  # tune shrinkage parameter lambda\n  cv.out = cv.glmnet(x = as.matrix(x = df[ , -which(names(df) == \"y\")]),\n                      y = df$y, alpha = 1, type.measure = \"deviance\")\n  \n  # run lasso selection on model using tuned lambda\n  lasso_mod = glmnet(x = as.matrix(x = df[ , -which(names(df) == \"y\")]),\n                      y = df$y, alpha = 1, lambda = cv.out$lambda.1se)\n  \n  # save names of non-shrunk X variables\n  lasso_vars =  names(lasso_mod$beta[,1])[which(lasso_mod$beta[,1] != 0)]\n  \n  # fit model based on lasso selected variables (plus intercept)\n  lasso_selected = lm(formula(paste0(\"y ~ 1 + \", paste(lasso_vars, collapse = \" + \"))), data = df)\n  \n  return(lasso_selected)\n\n}\n\n# test function on a single dataset\nlasso_var_mod(data_list$air_quality)\n\n\nCall:\nlm(formula = formula(paste0(\"y ~ 1 + \", paste(lasso_vars, collapse = \" + \"))), \n    data = df)\n\nCoefficients:\n (Intercept)      NMHC_GT_  PT08S2_NMHC_   PT08S3_NOx_       NO2_GT_  \n  -2.502e-17     6.240e-02    -9.494e-02     7.983e-02     7.019e-01  \n\n\nNow we need some functions to extract the results of interest from the selected model, i.e. collecting the measurements.\nThe outcome of the treatment is a model, so these functions should take in a model and output the measurement.\n\n# function for finding number of variables included\n# -&gt; inputs a model and returns an integer\nselect_var_count &lt;- function(lin_mod){\n  \n  # count the number of variables in the model (excluding intercept)\n  length(coef(lin_mod))-1\n  \n}\n\n# test function on a single dataset\nlasso_var_mod(data_list$air_quality) %&gt;% select_var_count\n\n[1] 4\n\n# function for finding 10-fold cross validated RMSE (our accuracy measure)\nselect_cv_rmse &lt;- function(lin_mod){\n  \n  # run 10-fold CV on the model\n  # -&gt; by default trainControl() uses bootstrap validation, so need to switch it\n  # -&gt; always want to use intercept, else it will try to tune the intercept (decide to include or not include it), the stepwise always gives an intercept so need fair comparison\n  cv_result = train(formula(lin_mod), \n                     data = lin_mod$model,\n                     method = \"lm\",\n                     trControl = trainControl(method = \"cv\", number = 10),\n                     tuneGrid = data.frame(intercept = TRUE))\n  \n  # return RMSE\n  return(cv_result$results$RMSE)\n}\n\n# test function on a single dataset\nlasso_var_mod(data_list$air_quality) %&gt;% select_cv_rmse\n\n[1] 0.7283729\n\n\nLots of work went into setting up how to apply the treatments and gathering results. Now we are just doing everything through functions.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computational Experiments</span>"
    ]
  },
  {
    "objectID": "computational-experiments.html#putting-together-the-experiment",
    "href": "computational-experiments.html#putting-together-the-experiment",
    "title": "5  Computational Experiments",
    "section": "\n5.5 Putting together the experiment",
    "text": "5.5 Putting together the experiment\n\n5.5.1 Run single trial\nWhile planning the steps above we might lose track of the general goal. Apply the treatments to the subjects and record the outcomes. With everything above setup well, actually running the experiment should be relatively simple.\nWe can still do this in a modularized fashion by first creating a function to run a single trial. In which, we don’t actually want to save the model that was fit, rather we want the results of it.\n\n# define function to run a single trial\n# -&gt; inputs each subject (df), applies the treatment (selection_alg which is a function), and collects the results\nrun_trial &lt;- function(selection_alg, df) {\n  \n  # run variable selection model\n  # -&gt; we can use a tmp prefix for the model to represent a temporary object (model) (it is temporary because it is in a temporary environment when the function is called)\n  tmp_mod = selection_alg(df)\n  \n  # collect measurements for number of variables and predictive accuracy\n  # -&gt; will be storing results as dataframe, so want to return a mini dataframe here\n  # -&gt; want to name elements when returning more complex data structures\n  return(data.frame(nvars = select_var_count(tmp_mod),\n                    rmse = select_cv_rmse(tmp_mod)))\n}\n\n# run trial for a single dataset to check results\nrun_trial(step_var_mod, data_list$air_quality)\n\n  nvars      rmse\n1     9 0.7257508\n\nrun_trial(lasso_var_mod, data_list$air_quality)\n\n  nvars      rmse\n1     4 0.7287472\n\n\n\n5.5.2 Run experiment\nNow we can run the experiment for all datasets. If trying to automate this, we can of coures use APPLY statements. When choosing which APPLY statement, we should think about what the input is (list of dataframes) and what the output will be (a dataframe).\n\n# run experiment\n# -&gt; run separately for stepwise and lasso algorithms\nresults_step &lt;- sapply(X = data_list, FUN = function(X) run_trial(step_var_mod, X))\nresults_lasso &lt;- sapply(X = data_list, FUN = function(X) run_trial(lasso_var_mod, X))\nresults_step\n\n      wpbc      wankara   laser     treasury   skillcraft puma      air_quality\nnvars 10        7         4         11         8          6         9          \nrmse  0.8867355 0.1065032 0.4861958 0.07009959 0.4911361  0.8827889 0.7258517  \n      ccpp      casp     \nnvars 4         9        \nrmse  0.2669818 0.8473485\n\nresults_lasso\n\n      wpbc      wankara   laser     treasury   skillcraft puma      air_quality\nnvars 4         5         4         7          6          1         4          \nrmse  0.8922084 0.1073599 0.4893904 0.07311562 0.4927779  0.8837042 0.7287855  \n      ccpp      casp     \nnvars 4         9        \nrmse  0.2669092 0.8473198\n\n\nIn most cases, stepwise selects a bigger model and has similar accuracy to LASSO.\n\n5.5.3 Questioning results\nBecause this is a computational experiment, we need to consider how the computer is behaving and which of our results should have some variability.\n\nIf we rerun the stepwise procedure, we should get the same results in the number of selected variables because it is a deterministic approach (it is calculating the AIC the same every single time). There is no randomness in this process. Although the accuracy (cv-rmse) measure may change a bit because may change slightly because of the cross validation. So we may need to repeatedly gather the cross-validated RMSE to see how uncertain we should be in those numbers (seeing if the accuracy is stable). We could simulate this 100 times and take the average cv-rmse; this would be the Monte Carlo simulation average of the 10-fold cross-validated RMSE. This would be a long term accuracy measure.\nIn the LASSO, there is an additional source of variation from the cross-validation procedure used to tune the shrinkage parameter \\(\\lambda\\) (data is randomly divided into 10-folds and then it iteratively goes through to find the best parameter value. How we divide up the data into 10 folds differs from trial to trial and we can get different results. This is where we start thinking about repetition and replication is necessary to understand uncertainty. Even though we are in a computational setting and have a lot more control, we are still working with outcomes and algorithms that have random components. So here, even though there isn’t randomization in our subjects, our treatments might manifest differently in different trials on the same data.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computational Experiments</span>"
    ]
  },
  {
    "objectID": "computational-experiments.html#improving-the-experiment",
    "href": "computational-experiments.html#improving-the-experiment",
    "title": "5  Computational Experiments",
    "section": "\n5.6 Improving the experiment",
    "text": "5.6 Improving the experiment\n\n5.6.1 Timing study\nWhen timing a study, we need to be careful about how this is done.\nThe best approach would be to build a function that not only fits the model based on the supplied selection procedure and dataframe, but also records the time it takes to do so.\nNote that assigning a start time and endtime is an imperfect way to record time because it takes the system time to record the time (assign the system time to the namespace, albeit a negligible amount).\nWe also want to be careful with the units for when we convert the time difference to numeric to store it. To account for this, use difftime() and specify the units for equal comparison. This is demonstrated below.\n\n# record start time\nstart_time &lt;- Sys.time()\n\n# run trial\nresults_lasso &lt;- sapply(X = data_list, FUN = function(X) run_trial(lasso_var_mod, X))\n\n# record end time\nend_time &lt;- Sys.time()\n\n# calculate difference - naive way\nend_time - start_time\n\nTime difference of 4.436627 secs\n\nas.numeric(end_time - start_time)\n\n[1] 4.436627\n\nend_time_step + 120 - start_time\n\nError in eval(expr, envir, enclos): object 'end_time_step' not found\n\nas.numeric(end_time + 120 - start_time)\n\n[1] 2.073944\n\n# have no idea what the units are when convert\n\n# calculate difference - correct way\ndifftime(end_time, start_time, units = \"sec\")\n\nTime difference of 4.436627 secs\n\nas.numeric(difftime(end_time, start_time, units = \"min\"))\n\n[1] 0.07394378\n\n\nNow we can incorporate timing into our function to run trials.\n\n# define function to run a single trial\n# -&gt; inputs each subject (df), applies the treatment (selection_alg which is a function), and collects the results\nrun_trial &lt;- function(selection_alg, df) {\n  \n  # run variable selection model\n  # -&gt; we can use a tmp prefix for the model to represent a temporary object (model) (it is temporary because it is in a temporary environment when the function is called)\n  # record start and end time\n  start_time = Sys.time()\n    tmp_mod = selection_alg(df)\n  end_time = Sys.time()\n  \n  # collect measurements for number of variables and predictive accuracy\n  # -&gt; will be storing results as dataframe, so want to return a mini dataframe here\n  # -&gt; want to name elements when returning more complex data structures\n  return(data.frame(nvars = select_var_count(tmp_mod),\n                    rmse = select_cv_rmse(tmp_mod),\n                    time = difftime(end_time, start_time, units = \"sec\")))\n}\n\n# run trial for a single dataset to check results\nrun_trial(lasso_var_mod, data_list$air_quality)\n\n  nvars      rmse          time\n1     4 0.7282684 0.182862 secs\n\n\nThen we can get Monte Carlo intervals for the timing.\nFinal note about timing: It is a good idea to test our experiments with small datasets like this to understand the behavior. For example, if one algorithm takes 5 times longer to run than another on just these smaller cases, then we might be worried about handing it 100,000 observations. If big datasets worry us, then perhaps simulated dataset is good to start with so we can control how large it is.\n\n5.6.2 Bundling the trial functions\nIn the implementation in Section 5.5.2, there is just a single function that runs the trials and we call if for both algorithms (applying it to all the datasets). We could setup another function to run the datasets through both algorithms and combine results.\n\n# define function to run both algorithms\nrun_both &lt;- function(df){\n  \n  # calculate results\n  # -&gt; add indicator for which algorithm was used\n  tmp_step = cbind(data.frame(algorithm = \"step\"),\n                   run_trial(step_var_mod, df))\n  tmp_lasso = cbind(data.frame(algorithm = \"lasso\"),\n                    run_trial(lasso_var_mod, df))\n\n  return(rbind(tmp_step, tmp_lasso))\n}\n\n# run all datasets through both algorithms\nresults &lt;- lapply(data_list, run_both)\nglimpse(results, max.level = 1)\n\nList of 9\n $ wpbc       :'data.frame':    2 obs. of  4 variables:\n $ wankara    :'data.frame':    2 obs. of  4 variables:\n $ laser      :'data.frame':    2 obs. of  4 variables:\n $ treasury   :'data.frame':    2 obs. of  4 variables:\n $ skillcraft :'data.frame':    2 obs. of  4 variables:\n $ puma       :'data.frame':    2 obs. of  4 variables:\n $ air_quality:'data.frame':    2 obs. of  4 variables:\n $ ccpp       :'data.frame':    2 obs. of  4 variables:\n $ casp       :'data.frame':    2 obs. of  4 variables:\n\nresults$air_quality\n\n  algorithm nvars      rmse           time\n1      step     9 0.7258746 0.6247990 secs\n2     lasso     4 0.7286026 0.1816499 secs\n\n\nWe could get better formatted results by adding an indicator for which dataset was used. Then we would be able to reduce the results list to a single dataframe rather than a list of smaller dataframes.\n\n5.6.3 Bootstrapping the data\nWe could also think about at this stage if the particular dataset is of interest or if we are more concerned about this type of data (data from a population like the 9 that are represented here). Reapplying to the same data wouldn’t get us anywhere because of the deterministic nature of the stepwise algorithm. So if the main question is how many variables tend to be selected in data like these, then we could bootstrap the dataframes going in (this would get at the variability in how large the set of variables tends to be selected in data like these, in addition to the variability in accuracies from dataset to dataset via the 10-fold cv-rmse). Then can get bootstrap intervals for the variable counts.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computational Experiments</span>"
    ]
  },
  {
    "objectID": "hw1.html#assignment",
    "href": "hw1.html#assignment",
    "title": "6  Homework 1",
    "section": "\n6.1 Assignment",
    "text": "6.1 Assignment\nYour task for this week is simple in concept but challenging to code efficiently. You need to simulate samples from gamma distributions using different sample sizes, different shape parameters and different scale parameters. You will then use these simulated values to compute sample means and 95% Confidence Intervals (for the population mean) for each of the samples that you simulated. You must do so by building the set of functions and set of data objects that match the details in the steps below.\n\n6.1.1 Gather simulated gamma data\nFirst part:\nBuild a function named sim_gam that has formal arguments for sample size (n), the vector of shapes (length M), the vector of scales (length L). Also include an argument that will allow the user to add a random seed number for reproducible simulation, but ignore if not specified. The body of you function must use these arguments to generate a 3-dimensional array of simulated values from a gamma distribution. The size and indexing in each dimension in this array should be (n) by (M) by (L).\nYour array should also have the “dimnames” attribute of the array added for the shape and scale dimensions that contain labels related to the parameter values that those dimensions represent.\nSecond part:\nUse your gamma simulation function to create a list of arrays called gam_arrays_list with the following attributes:\n\nAll arrays should include results for shape_vec = c(0.1, 1, 10, 100) and scale_vec = c(0.1, 1, 10, 100)\nEach array will represent a different sample size: n = c(25, 50, 75, 100, 500,1000)\n\n6.1.2 Compute means and confidence intervals\nFirst part:\nNext build the following two functions that the output array from the gamma data simulator above, as their inputs: 1. A function named gam_means that returns an (M) by (L) matrix of means from the simulated values from each shape and scale parameter pair.\n\nA function named gam_CIs that returns a (2) by (M) by (L) array containing the lower and upper confidence bounds for the 95% confidence intervals for the means of each simulated gamma sample.\n\nHints\n\nCheck out the apply() function help documentatio.n\nYou will likely need a helper function within your gam_CIs function that can take in a sample vector and return the 95% Confidence Interval as a vector of c(lower.val, upper.val). I suggest looking at the results of the t.test() function for some prebuilt access to 95% CI for means.\n\nSecond part:\nApply your gam_means() function to your simulated data in gam_arrays_list. Store the results as list of means matrices\nAlso, apply your gam_CIs() function to your simulated data in gam_arrays_list. Store the results an additional list of Confidence Interval arrays.",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#practice",
    "href": "hw1.html#practice",
    "title": "6  Homework 1",
    "section": "\n6.2 Practice",
    "text": "6.2 Practice\nHere are some demos of working with simple examples to see how APPLY statements and MAP statements work on data types (matrix vs dataframe).\n\nlibrary(tidyverse)\n\n# investigate data types and vectorized results\n\n# example 2D dataset as different data structure\ndata_mat &lt;- matrix(data = rnorm(1000), nrow = 200, ncol = 5)\ndata_df &lt;- data.frame(data_mat)\n\n# loop over columns to find means\n\n# BEST WAY\n# -&gt; should be zeros, works for both types\napply(X = data_mat, MARGIN = 2, FUN = mean)\n\n[1] -0.05400396 -0.05404391  0.11716543  0.04152527  0.03911136\n\nsapply(data_df, mean) # works because each column in a dataframe is like a list and simplifies result\n\n         X1          X2          X3          X4          X5 \n-0.05400396 -0.05404391  0.11716543  0.04152527  0.03911136 \n\ndata_df %&gt;% map_dbl(mean)\n\n         X1          X2          X3          X4          X5 \n-0.05400396 -0.05404391  0.11716543  0.04152527  0.03911136 \n\n# OTHER CORRECT WAYS\nsapply(X = 1:ncol(data_mat), function(X) mean(data_mat[,X]))\n\n[1] -0.05400396 -0.05404391  0.11716543  0.04152527  0.03911136\n\napply(X = data_df, MARGIN = 2, FUN = mean)\n\n         X1          X2          X3          X4          X5 \n-0.05400396 -0.05404391  0.11716543  0.04152527  0.03911136 \n\nsapply(X = 1:ncol(data_df), function(X) mean(data_df[,X]))\n\n[1] -0.05400396 -0.05404391  0.11716543  0.04152527  0.03911136\n\nlapply(data_df, mean)\n\n$X1\n[1] -0.05400396\n\n$X2\n[1] -0.05404391\n\n$X3\n[1] 0.1171654\n\n$X4\n[1] 0.04152527\n\n$X5\n[1] 0.03911136\n\ndata_df %&gt;% map_vec(mean) # know returning data type, so should use _dbl\n\n         X1          X2          X3          X4          X5 \n-0.05400396 -0.05404391  0.11716543  0.04152527  0.03911136 \n\ndata_df %&gt;% map(mean) %&gt;% reduce(c) # reduce cause map() returns a list\n\n[1] -0.05400396 -0.05404391  0.11716543  0.04152527  0.03911136\n\n# INCORRECT WAYS\nhead(sapply(data_mat, mean), n = 20) # this takes each individual value and returns the mean (so the same value)\n\n [1]  0.47396250 -0.55306786  0.35162823 -0.76858673 -2.28475143 -0.10665379\n [7] -0.71853290 -0.44057683  1.22256012 -1.05750451  0.13752910 -0.04898068\n[13]  0.70449112  0.87378153 -0.84670521 -0.74373214 -1.30336909  1.77780881\n[19] -0.62932110  0.83923256\n\nhead(sapply(data_mat, mean), n = 10) == data_mat[1:10,1]\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nhead(data_mat %&gt;% map(mean)) # same thing\n\n[[1]]\n[1] 0.4739625\n\n[[2]]\n[1] -0.5530679\n\n[[3]]\n[1] 0.3516282\n\n[[4]]\n[1] -0.7685867\n\n[[5]]\n[1] -2.284751\n\n[[6]]\n[1] -0.1066538\n\n\nNow investigate structure of data holder.\n\n# this is just to make that we can correctly summarize over columns and layers of an array and keep index names for the result\n\n# fill example 2D dataset as a matrix\ndata_mat &lt;- matrix(data = rnorm(100), nrow = 20, ncol = 5)\n\n# change dimensions and add dimension label\ndim(data_mat) &lt;- c(10, 5, 2)\ndimnames(data_mat) &lt;- list(paste0(\"x\", 1:10), paste0(\"y = \", 1:5), paste0(\"z = \", 1:2))\n\n# summarize over rows: this averages over x for each combination of y and z\n# check result\ndata_means &lt;- apply(data_mat, MARGIN = c(2, 3), mean)\ndata_means[\"y = 1\", \"z = 1\"] == mean(data_mat[ , \"y = 1\", \"z = 1\"])\n\n[1] TRUE\n\n\nNow apply confirmed results from above to a real scenario.\n\n# initial vectors\n# -&gt; i = obs ID, mu = mean and sigma = sd\ni &lt;- 1:10 # not using n = 10 so that can set row names below\nmu &lt;- c(0, 5, 10, 100, 500)\nsigma &lt;- c(0.5, 5)\n\n# initialize an empty array (dim = i by mu by sigma) and change dimension labels\ndata_sim &lt;- array(rep(NA, length(i) * length(mu) * length(sigma)),\n                  dim = c(length(i), length(mu), length(sigma)))\ndimnames(data_sim) &lt;- list(paste0(\"i\", i), paste0(\"mu = \", mu), paste0(\"sigma = \", sigma))\n\n# to generate the data in the best way, need to be saving just the results from the iterations i, and double for loop to access the different columns (mean) and layers (sd) like in the notes for beyond dataframe\n\n# vectorize over the means to create a matrix of obs from each mean\n# then use a for loop to loop over the different standard deviation (as the third dimension)\n# (use the holding structure 'data' that is create at beginning of this section)\nfor (k in 1:length(sigma)) {\n  for (j in 1:length(mu)) {\n    data_sim[ , j, k] &lt;- rnorm(n = length(i), mean = mu[j], sd = sigma[k])\n  }\n}\n\n# now confirm that the summaries match the parameters\ndata_means &lt;- apply(data_sim, MARGIN = c(2, 3), mean)\ndata_sd &lt;- apply(data_sim, MARGIN = c(2, 3), sd)\ndata_means\n\n          sigma = 0.5  sigma = 5\nmu = 0    -0.06373573   1.397530\nmu = 5     5.07286903   4.643620\nmu = 10   10.08404702   9.772814\nmu = 100 100.18075617 101.300889\nmu = 500 500.11772703 497.297924\n\ndata_sd\n\n         sigma = 0.5 sigma = 5\nmu = 0     0.4303220  4.238961\nmu = 5     0.4062227  4.895236\nmu = 10    0.7479058  4.296652\nmu = 100   0.5462804  5.341503\nmu = 500   0.4054104  3.440882",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#solution",
    "href": "hw1.html#solution",
    "title": "6  Homework 1",
    "section": "\n6.3 Solution",
    "text": "6.3 Solution\n\n6.3.1 Gather simulated gamma data\n\n# define function to create an array [sample size (n) x # of shapes (M) x # of scales (L)] of simulated data from a gamma distribution\n# -&gt; arguments: n = sample size (integer), M = shape parameters (numeric vector), scale parameters (numeric vector), seed to set (integer)\n# -&gt; returns: n x M x L array\nsim_gam &lt;- function(sample_size = 10, shapes = 1, scales = 1, seed = NULL) {\n  \n  # conditionally set the random seed\n  if (!is.null(seed))\n    set.seed(seed)\n  \n  # initialize an empty array (dim = n by shape by scale) and change main dimension labels\n  data_sim = array(rep(NA, sample_size * length(shapes) * length(shapes)),\n                  dim = c(sample_size, length(shapes), length(shapes)))\n  dimnames(data_sim) = list(NULL, paste0(\"shape = \", shapes), paste0(\"scale = \", scales))\n  \n  # generate data from the specified shape and scale combination using nested for loops\n  # NOTE: there are different ways to do this with either two sapply() statements or a for loop with a sapply\n  # -&gt; but this is makes the most logical sense: fill each layer column by column rather than fill one layer \"in bulk\"\n  for (k in 1:length(scales)) {\n    for (j in 1:length(shapes)) {\n      data_sim[ , j, k] = rgamma(n = sample_size, shape = shapes[j], scale = scales[k])\n    }\n  }\n  \n  return(data_sim)\n}\n\nThe body from the above implementation is where we generate the data is equivalent to the following if we wanted to use an APPLY statement.\n\n  # equivalent to:\n  # use an inner sapply() to fill the entire layer with one function call (sapply() then does it shape by shape column at a time) rather than column by column loop for changing layer\n  for (i in seq_along(scales)) {\n\n    # generate the random numbers according to the scales (changes by matrix) and the shapes (changes by column within matrix)\n    data_sim[ , , i] &lt;- sapply(X = shapes, function(X) rgamma(n = sample_size, shape = X, scale = scales[i]))\n  }\n\nNow with the function defined, we can generate the data.\n\n# initial vectors of the changing parameters for generating data\nsample_sizes &lt;- c(25, 50, 75, 100, 500, 1000)\nshape_vec &lt;- c(0.1, 1, 10, 100)\nscale_vec &lt;- c(0.1, 1, 10, 100)\n\n# generate data\n# add dimension labels for the sample sizes\ngam_arrays_list &lt;- sapply(X = sample_sizes, function(X) sim_gam(sample_size = X, shapes = shape_vec, scales = scale_vec))\nnames(gam_arrays_list) &lt;- paste0(\"n = \", sample_sizes)\n\n# view results\nstr(gam_arrays_list)\n\nList of 6\n $ n = 25  : num [1:25, 1:4, 1:4] 8.55e-05 6.33e-06 7.80e-08 4.98e-05 1.97e-07 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : NULL\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 50  : num [1:50, 1:4, 1:4] 2.52e-02 1.43e-02 1.98e-06 1.78e-08 1.04e-02 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : NULL\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 75  : num [1:75, 1:4, 1:4] 1.31e-05 3.77e-13 2.26e-08 4.55e-15 5.52e-04 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : NULL\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 100 : num [1:100, 1:4, 1:4] 6.25e-06 1.07e-04 9.08e-17 1.69e-04 2.12e-08 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : NULL\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 500 : num [1:500, 1:4, 1:4] 1.22e-03 5.13e-02 1.97e-07 4.20e-02 6.54e-03 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : NULL\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 1000: num [1:1000, 1:4, 1:4] 6.21e-02 4.90e-03 8.46e-03 5.20e-07 8.84e-03 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : NULL\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n\nround(gam_arrays_list$`n = 25`[, , 1], 3)\n\n      shape = 0.1 shape = 1 shape = 10 shape = 100\n [1,]       0.000     0.014      1.019       9.833\n [2,]       0.000     0.096      1.391       9.837\n [3,]       0.000     0.016      1.123      11.214\n [4,]       0.000     0.121      1.494       9.199\n [5,]       0.000     0.073      1.077      10.242\n [6,]       0.076     0.025      0.998      10.080\n [7,]       0.009     0.225      1.069       8.681\n [8,]       0.010     0.034      0.656       9.161\n [9,]       0.000     0.006      1.308       8.565\n[10,]       0.000     0.011      1.324      10.208\n[11,]       0.008     0.090      0.782       8.785\n[12,]       0.000     0.004      0.963       9.483\n[13,]       0.024     0.166      0.704      10.095\n[14,]       0.000     0.150      1.433       9.328\n[15,]       0.000     0.049      0.944       9.357\n[16,]       0.013     0.148      1.364       9.614\n[17,]       0.000     0.030      0.972      10.391\n[18,]       0.001     0.129      1.051       8.613\n[19,]       0.003     0.008      1.031      10.914\n[20,]       0.000     0.065      0.546       8.885\n[21,]       0.058     0.020      1.099       9.876\n[22,]       0.000     0.076      0.767      11.827\n[23,]       0.037     0.087      1.811       8.764\n[24,]       0.000     0.056      0.683      10.997\n[25,]       0.012     0.037      0.797      10.553\n\n\n\n6.3.2 Compute means and confidence intervals\n\n# define function to calculate the mean of the simulated observations for each combination of shape and scale\n# -&gt; argument: n x M x L array of simulated values\n# -&gt; returns: M x L matrix of column means from each layer\ngam_means &lt;- function(gam_array){\n  apply(gam_array, MARGIN = c(2, 3), mean)\n}\n\n# function to get CIs for each combination of shape and scale parameters\n# -&gt; argument: n x M x L array of simulated values\n# -&gt; returns: 2 x M x L array of confidence interval bounds for each column of each layer\ngam_CIs &lt;- function(gam_array){\n  \n  # run a t-test for each combination of shape and scale values\n  t_tests = apply(X = gam_array, MARGIN = c(2, 3), FUN = t.test)\n  \n  # extract the CI bounds for the mean of the samples\n  ci_bounds = apply(X = t_tests, MARGIN = c(1, 2), function(t) t[[1]]$conf.int)\n  \n  # add rownames for each matrix to indicate which CI bound it is\n  dimnames(ci_bounds)[[1]] &lt;- c(\"lower bound\", \"upper bound\")\n  \n  # return array\n  return(ci_bounds)\n}\n\n# (multiple ways to) create a list of arrays containing the means for each sample of the generated data\n# use lapply to keep as list and not simplify to matrix\ngam_x_bars &lt;- lapply(gam_arrays_list, gam_means) # equivalent to: lapply(X = gam_arrays_list, FUN = function(X) gam_means(X))\ngam_x_bars &lt;- map(gam_arrays_list, \\(array) gam_means(array)) # more readable\n\n# create a list of arrays containing the confidence interval bounds for the mean of each sample of the generated data\ngam_ci_bounds &lt;- map(gam_arrays_list, \\(array) gam_CIs(array))\n\n# view results\nstr(gam_x_bars)\n\nList of 6\n $ n = 25  : num [1:4, 1:4] 0.01 0.0695 1.0562 9.7801 0.0475 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 50  : num [1:4, 1:4] 0.00405 0.10392 1.02116 9.97883 0.08361 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 75  : num [1:4, 1:4] 0.0125 0.0954 1.0022 10.078 0.0859 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 100 : num [1:4, 1:4] 0.0075 0.1085 1.0555 10.0528 0.1657 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 500 : num [1:4, 1:4] 0.0111 0.0954 0.9875 10.0186 0.0797 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 1000: num [1:4, 1:4] 0.0112 0.1011 0.9871 9.9557 0.1007 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n\ngam_x_bars$`n = 25`\n\n            scale = 0.1    scale = 1  scale = 10 scale = 100\nshape = 0.1  0.01001787   0.04751898   0.7051746    12.78012\nshape = 1    0.06949849   0.91883925   9.7653604   112.31653\nshape = 10   1.05623999  10.09542504  92.9124727   985.30090\nshape = 100  9.78008855 103.52050976 956.6786584 10364.17523\n\nstr(gam_ci_bounds)\n\nList of 6\n $ n = 25  : num [1:2, 1:4, 1:4] 0.00199 0.01805 0.04506 0.09394 0.9314 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:2] \"lower bound\" \"upper bound\"\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 50  : num [1:2, 1:4, 1:4] 0.00185 0.00624 0.07613 0.13171 0.92321 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:2] \"lower bound\" \"upper bound\"\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 75  : num [1:2, 1:4, 1:4] 0.00044 0.02463 0.07095 0.11984 0.92489 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:2] \"lower bound\" \"upper bound\"\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 100 : num [1:2, 1:4, 1:4] 0.00282 0.01218 0.08727 0.12968 0.99306 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:2] \"lower bound\" \"upper bound\"\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 500 : num [1:2, 1:4, 1:4] 0.00847 0.01366 0.08708 0.10373 0.95978 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:2] \"lower bound\" \"upper bound\"\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n $ n = 1000: num [1:2, 1:4, 1:4] 0.00885 0.01353 0.09451 0.10775 0.9669 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:2] \"lower bound\" \"upper bound\"\n  .. ..$ : chr [1:4] \"shape = 0.1\" \"shape = 1\" \"shape = 10\" \"shape = 100\"\n  .. ..$ : chr [1:4] \"scale = 0.1\" \"scale = 1\" \"scale = 10\" \"scale = 100\"\n\ngam_ci_bounds$`n = 25`\n\n, , scale = 0.1\n\n            shape = 0.1  shape = 1 shape = 10 shape = 100\nlower bound 0.001988243 0.04506035  0.9314015    9.415955\nupper bound 0.018047502 0.09393663  1.1810785   10.144222\n\n, , scale = 1\n\n            shape = 0.1 shape = 1 shape = 10 shape = 100\nlower bound -0.01023031 0.6460409   8.887475    98.75415\nupper bound  0.10526827 1.1916376  11.303375   108.28687\n\n, , scale = 10\n\n            shape = 0.1 shape = 1 shape = 10 shape = 100\nlower bound   0.2265346  5.628111   79.98262    922.0426\nupper bound   1.1838147 13.902610  105.84233    991.3147\n\n, , scale = 100\n\n            shape = 0.1 shape = 1 shape = 10 shape = 100\nlower bound   -2.026462  77.09184   883.8548    9939.587\nupper bound   27.586708 147.54122  1086.7470   10788.764",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "permutation-tests.html#fishers-exact-test",
    "href": "permutation-tests.html#fishers-exact-test",
    "title": "3  Permutation Tests",
    "section": "\n3.1 Fisher’s Exact Test",
    "text": "3.1 Fisher’s Exact Test\nThe idea of permutation testing grows out of Fisher’s Exact test; part of the experimental design work by R.A. Fisher at Rothemstad in the 1920’s. The canonical example of Fisher’s exact test is the “Lady Tasting Tea” experiment. As the story goes, Muriel Bristol was a lady who worked with Fisher at Rothemstad. She claimed to be able to tell if a cup of tea was made with milk first then tea added, or tea first then milk added. Fisher was suspicious and devised an experiment to test her skills. 8 cups of tea were made: 4 milk first, 4 tea first. The eight cups were randomized in order, an Muriel was asked to sort them through a blinded taste test.\nIf she was guessing, then the number of correct guesses would follow a Hypergeometric distribution. (Contemporaneous accounts say that she got all 8 correct – thus a p-value = 0.0143). This problem is essentially just a combinatorics problem. It is the same as choosing colored balls from an urn, for example 4 white and 4 black. What is the chance you reach in and get all the same color?\nWith Fishers exact test, if the null hypothesis is true that we are just guessing, we know the number exact number of ways to be correct based on the theoretical hypergeometric distribution. This means that the hypergeometric is effectively a combinatoric problem, looking through the number of possible ways to see a certain set of outcomes out of the number of possible ways total.\n\nUsing R notation: \\(\\displaystyle P(X = x) = \\frac{{m \\choose x}{n \\choose k-x}}{{m + n\\choose k}}\\), where \\(m\\) = number of objects of interest, \\(n\\) = number of objects not of interest, \\(k\\) = number that we are selecting, and \\(x\\) is the number of objects of interest selected.\nUsing alternate notation: \\(\\displaystyle P(X = x) = \\frac{{M \\choose x} {N - M \\choose K - x}}{{N \\choose K}}\\), where \\(N\\) = population size and \\(M\\) = objects of interest (others are the same).\n\nIn the tea problem \\(m\\) is the number of cups of milk first, \\(n\\) is the number of cups of tea first, and \\(k\\) is the number of cups for guessing “tea first”. For the 8 cups, this would mean that there was only one way to guess them all correctly out of the \\({(4 + 4) \\choose 4}\\) = 70 possible ways to guess 4 cups; thus 1 / 70 = 0.0143 p-value stated earlier.\nThe exact test assumes there is a closed-form distribution (shape) that we can get the p-value from, so knowing what all possible outcomes looks like as a distribution. It exactly follows a hypergeometric. With means for example, we could assume some asymptotic properties of the CLT, but that is no longer an exact test.\nPsuedo ranking of tests:\n\nExact test (cleanest, least practical): Nice when have situation that lends itself nicely to a closed-form solution. Don’t need to check all possible combinations because can just calculate from the known theoretical distribution.\nPermutation test (less clean, more practical): We don’t know the closed form of the statistic for all possible combinations, so it’s not an exact test. But we know the empirical distribution from the exhaustive check of all possible combinations (group assignments), and this is how we get information is by comparing the summary for every assignment.\nMonte Carlo simulation (least clean, actually practical): Most realistic setting, very complicated scenarios in practice. Almost never going to be able to get a closed-form solution with complex summary statistics and datasets are usually too large to do full permutation tests.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "permutation-tests.html#extending-to-non-binary-outcomes",
    "href": "permutation-tests.html#extending-to-non-binary-outcomes",
    "title": "3  Permutation Tests",
    "section": "\n3.2 Extending to non-binary outcomes",
    "text": "3.2 Extending to non-binary outcomes\nBut how well does this generalize to non-binary outcome (e.g. comparison of means, where we aren’t just concerned about correct or not correct)? Do the theoretically properties always stay simple with nice closed form combinatoric solutions?\nConsider the simple case where we have 50 people randomly assigned to two different drug groups and we want to test if the average blood pressure from Drug 1 is higher the average from Drug 2. We will have data with blood pressures \\(Y_{ij}\\) for person \\(i \\in {1, 2, \\ldots, 50}\\) for drug \\(j \\in {1,2}\\). If the difference in means was \\(\\bar{Y_1} - \\bar{Y_2} = 20\\) mmHG how would we determine a p-value?\n\n3.2.1 Parametric (typical approach)\n\nRun a t-test with the two samples to compare the means (after checking the assumptions of course).\nIn order to get a p-value, we need a test statistic. Specifically we need the sampling distribution of the test statistic, found via theoretical statistics.\nAssumptions: \\(\\bar{Y} \\sim \\text{Normal}\\) by CLT (\\(n = 50\\)), maybe equal variances \\(\\sigma_1^2 = \\sigma_2^2\\).\nIf assumptions are met \\(\\Longrightarrow\\) Then can get difference in means \\(\\bar{Y_1} - \\bar{Y_2} \\sim t_{98}\\) and can then get the p-value. This is all based on how means behave sample to sample evaluated with theoretical results.\n\n3.2.2 Permutation Testing\n\nInstead of assuming theoretical properties of the distribution of differences, an alternative approach that is closer to Fisher’s exact test is called permutation testing.\nIf we assume the null is true, then the random assignment of people to treatment groups is the source of the randomness in the results (the only distinguishing factor is the group label then).\nTo rework the difference in means hypothesis test, we could:\n\n\nFigure out what should happen if the NULL of no difference is true. The difference in blood pressure averages of 20 mmHg occurred by random chance due to the random assignment of 50 people to each group; this is the null.\nFigure out how many possible outcomes there are? \\({100 \\choose 50}\\) = 1.0089134^{29}.\nYikes, that is a lot of possible outcomes! How many would lead to an outcome that is as extreme or more extreme than we observed (difference \\(\\ge\\) 20 mmHg)?\n\n\nUnknown, so we just need to compute for all 1.0089134^{29} combinations of people… (but obviously we do not have that kind of computing power / time). This would entail reassigning labels in ALL possible ways and calculating the sample means of each new group and looking at the difference in means.\n\n3.2.3 Monte Carlo Testing\n\nAs a compromise, we can use Monte Carlo to check a random subset of the total number of ways (for example, randomly order 10,000 times). If we do this enough times, it should mimic a permutation test where we exhaustively check all possible ways.\nThen in the simulation, look how often get a test statistic more extreme than what was observed? This is at it’s core a p-value if we convert it to a probability.\n\nMore formal steps:\n\nFigure out what should happen if the NULL of no difference is true (from context experts). The difference in blood pressure averages of 20 mmHg occurred by random chance due to the random assignment of 50 people to each group.\nHow many would lead to an outcome that is as extreme or more extreme than we observed (difference \\(\\ge\\) 20 mmHg)? Simulate the random assignment of people to treatments by permuting the drug labels in your data set. Record the difference in averages. Then repeat this process a reasonable number of times. Perhaps M=10000 times. Calculate the p-value as the number of random permutations out of your trials where the average difference exceeded 20 mmHg.\n\n\nSo in summary, Monte Carlo approach estimates a permutation test with simulation and computation to get a p-value, rather than theoretically finding the results by using an exhaustive list of options.\nHow many simulations are sufficient? The number of permutations that is sufficient is determined by your time frame (this is the main way in practice). Also it depends on how much precision we want when estimating the p-value (i.e. if only have 100 simulations, we can only estimate the p-value to the hundredths place; so if wanted to the thousandths place, would need 1000 simulations, and so on). Just want to be confident that we have captured the long term behavior of the scenario and still get the inference in a reasobable amount of time. This is a function of how much variability you expect from sample-to-sample.\n\n3.2.4 Comparison of methods\n\nIs there a preference in terms of parametric vs permutation tests in which is better? Pros and cons\nANSWER: Permutation testing is when you are able to simulate from the model under the null and you are able to use it as a generative function (generate data from the assumed model), then the permutation test will be a more direct representation of the test than test statistic / parametric methods that usually hinge on assumptions that are usually loosely true. So permutation tests are more pure in what p-values represent, the percentage of samples that would result in a significant difference.\n\n3.2.5 Permutation Test Example:\nUsing data from the student sleep data. The following is directly taken from the help file for the sleep data in Base R:\nA data frame with 20 observations on 3 variables.\n[, 1] extra numeric increase in hours of sleep\n[, 2] group factor drug given\n[, 3] ID factor patient ID\nWe could do a t-test, but it is a small scenario. So, let’s see if we can exhaustively serve possible permutations.\n\nEven with only 20 observations, still a very large number of possible permutations. Probably won’t be able to run an exhaustive permutation test if the number is over a million or so\n\n\nsleep\n\n   extra group ID\n1    0.7     1  1\n2   -1.6     1  2\n3   -0.2     1  3\n4   -1.2     1  4\n5   -0.1     1  5\n6    3.4     1  6\n7    3.7     1  7\n8    0.8     1  8\n9    0.0     1  9\n10   2.0     1 10\n11   1.9     2  1\n12   0.8     2  2\n13   1.1     2  3\n14   0.1     2  4\n15  -0.1     2  5\n16   4.4     2  6\n17   5.5     2  7\n18   1.6     2  8\n19   4.6     2  9\n20   3.4     2 10\n\n# calculate the number of possible permutations from random assignment\n# -&gt; exact permutation tests become scary realzzz fast\nchoose(20,10)\n\n[1] 184756\n\n\nUse utils::combn() function to define matrix of all sets of indices; the result is a \\(k \\times {n \\choose k}\\) matrix where each column is the indices of the selected objects for that permutation.\n\n# build a permutation matrix\n# ?combn\nsleep_sets &lt;- combn(1:20, m = 10)\ndim(sleep_sets)\n\n[1]     10 184756\n\n# preview matrix\n# -&gt; a column of this sleep_sets is a vector of length 10 for the indices of the observations (from the original data) that are in the first treatment group for this permutation \nsleep_sets[, 1:3]\n\n      [,1] [,2] [,3]\n [1,]    1    1    1\n [2,]    2    2    2\n [3,]    3    3    3\n [4,]    4    4    4\n [5,]    5    5    5\n [6,]    6    6    6\n [7,]    7    7    7\n [8,]    8    8    8\n [9,]    9    9    9\n[10,]   10   11   12\n\n\nNow we can calculate group means using with().\n\nThis function acts like using a dollar sign to access columns (i.e. nested naming such as df$col).\nIt creates a temporary environment and attaches all columns names to the data that is given so we don’t have to have the nested names each time.\n\n\n# demo with\n# -&gt; extract just the first groups' `extra` column values\n# -&gt; response is `extra` column, and `group` is the labels\nwith(sleep, extra[group == 1]) # equivalent to sleep$extra[sleep$group == 1]\n\n [1]  0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0\n\n# calculate difference in means of two groups\n# -&gt; psuedocode: for sleep matrix, calculate the of mean of extra column for group 1 and subtract the mean of group two\nwith(sleep, mean(extra[group == 1]) - mean(extra[group == 2]))\n\n[1] -1.58\n\n\nIs this value significant? We could use parameterics. This difference would be be used to calculate the t-stat or be center of a t-interval and could determine significance from that.\n\n# check assuptions \nhist(sleep$extra[sleep$group == 1])\n\n\n\n\n\n\n\n\nhist(sleep$extra[sleep$group == 2])\n\n\n\n\n\n\n\n\n# -&gt; decent enough\n\n# run two-sample t test\nt.test(x = sleep$extra[sleep$group == 1], y = sleep$extra[sleep$group == 2])\n\n\n    Welch Two Sample t-test\n\ndata:  sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2]\nt = -1.8608, df = 17.776, p-value = 0.07939\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.3654832  0.2054832\nsample estimates:\nmean of x mean of y \n     0.75      2.33 \n\n\nOr we can do a permutation test. To do this, we need a function can take a vector of indices, and return the average difference in sleep in the two groups.\nFunction writing strategy: Start with a specific call that we want (using with()), then generalize by making it work for non-hardcoded names.\n\n# hardcoded what we want\nwith(sleep, mean(extra[1:10] - mean(extra[-(1:10)])))\n\n[1] -1.58\n\n# convert to function indices of a matrix\nsleep_compare &lt;- function(idx_vec) {\n  with(sleep, mean(extra[idx_vec] - mean(extra[-(idx_vec)])))\n}\n\n# loop over indices of a matrix\n# -&gt; sleep_sets has the labels down the columns (column = the sets of indices), this is what we want to traverse (iterate, loop over)\n# -&gt; so give each column of sleep_sets to the function sleep_compare\nperm_diffs &lt;- apply(X = sleep_sets, MARGIN = 2, FUN = sleep_compare)\n\n# look at results\nhead(perm_diffs)\n\n[1] -1.58 -1.60 -1.82 -1.76 -1.96 -2.00\n\nhist(perm_diffs)\n\n\n\n\n\n\n\n\nround(quantile(perm_diffs, seq(from = 0.1, to = 0.9, by = 0.1)), 3) # see if distribution is balanced around zero actually\n\n  10%   20%   30%   40%   50%   60%   70%   80%   90% \n-1.18 -0.78 -0.48 -0.24  0.00  0.24  0.48  0.78  1.18 \n\n# finer quantiles (trying to get closer to the observed difference)\nround(quantile(perm_diffs, seq(from = 0.025, to = 0.975, by = 0.05)), 3)\n\n 2.5%  7.5% 12.5% 17.5% 22.5% 27.5% 32.5% 37.5% 42.5% 47.5% 52.5% 57.5% 62.5% \n-1.76 -1.32 -1.06 -0.86 -0.70 -0.56 -0.42 -0.30 -0.18 -0.06  0.06  0.18  0.30 \n67.5% 72.5% 77.5% 82.5% 87.5% 92.5% 97.5% \n 0.42  0.56  0.70  0.86  1.06  1.32  1.76 \n\n\nThis tells us if the null hypothesis is true and we are randomly assigning group labels, then the difference in sleep should be zero (no difference). Let’s calculate a p-value by using the permuted differences compared to the real difference that the study found. Specifically we can count the number of permutations further out in the tail than the real difference and get that proportion.\n\n# save real difference\nreal_diff &lt;- with(sleep, mean(extra[group == 1] - extra[group == 2]))\n\n# calculate a p-value\n# -&gt; proportion of values where condition is true -&gt; mean(condition)\n# this is a one tail p-value\nsum(perm_diffs &lt;= real_diff) / length(perm_diffs)\n\n[1] 0.04072398\n\nmean(perm_diffs &lt;= real_diff)\n\n[1] 0.04072398\n\n# two tailed\nmean(abs(perm_diffs) &gt;= abs(real_diff))\n\n[1] 0.08141008\n\n\nAssumptions / basic building blocks of this permutation test (what did we have to do?):\n\nHad count the number of permutations, and keep track of them (not much of an assumption)\nNeed a way to summarize the data (not much of an assumption), some summary statistic.\nNeed to have the structure of hypothesis test ahead of time: one-tailed, two-tailed, significance level to compare the p-value to.\nOnly fundmental assumption was that there is a summary statistic for which there is a meaningful compariosn to be made and that we can mimic the randomization by simulation.\n\n3.2.6 Monte Carlo Simulated Permutation Test Example:\nMonte Carlo methods are the idea that if you can simulate data from the assumed model, you can simulate Monte Carlo samples (simulated outcomes from that model). And if there is some property that those samples should have, we can store and evaluate those in a Monte Carlo simulation and studying their behavior.\nUse bigger data set where exhaustive permutation wouldn’t be possible. Try testing if the price of a high end home (at the 90th percentile) is higher for homes with or without swimming pools. Definitely can’t use a t-distribution for comparing quantiles nor figure out how the sampling distribution behaves theoretically, so this is a benefit of the simulation approach: can compare a wider variety of statistics.\nUse the Ames homes data.\n\n# load the real estate data\nreal_estate &lt;- read.csv(\"Files/Data/realestate.csv\")\n\n# check out the data\nhead(real_estate)\n\n   price sqft bed bath  ac cars pool built quality style   lot highway\n1 360000 3032   4    4 yes    2   no  1972  medium     1 22221      no\n2 340000 2058   4    2 yes    2   no  1976  medium     1 22912      no\n3 250000 1780   4    3 yes    2   no  1980  medium     1 21345      no\n4 205500 1638   4    2 yes    2   no  1963  medium     1 17342      no\n5 275500 2196   4    3 yes    2   no  1968  medium     7 21786      no\n6 248000 1966   4    3 yes    5  yes  1972  medium     1 18902      no\n\ntable(real_estate$pool)\n\n\n no yes \n486  36 \n\n# difference in 90th percentiles\n# -&gt; overall and then with / without swimming pool\nquantile(real_estate$price, 0.9)\n\n   90% \n489003 \n\n# another with statement\nwith(real_estate, quantile(price[pool == \"yes\"], 0.9))\n\n   90% \n549950 \n\nwith(real_estate, quantile(price[pool == \"no\"], 0.9))\n\n   90% \n478500 \n\n\nNow make a function that randomly permutes the pool labels then compares quantiles.\n\nNote that that there are different strategies for setting this up. In the first example, a UDF calculated the difference and apply() did the permuting. Now we will have the UDF do both jobs and we just need to loop over the number of simulations we are going to run.\nFor loops have been optimized in R, so we are not really losing efficieny in terms of computing time by not using APPLY statements. It just depends on how you want to read the code.\n\n\n# we have no idea how the difference in 90th quantile of house prices behaves theoretically, but we can compute it\nwith(real_estate, quantile(price[pool == \"yes\"], 0.9) - quantile(price[pool == \"no\"], 0.9))\n\n  90% \n71450 \n\n# now we need to permute the labels\n# -&gt; to create a random order of the houses and keep the sample sizes for each group (of pool), we can sample without replacement\nhead(real_estate$pool, n = 20)\n\n [1] \"no\"  \"no\"  \"no\"  \"no\"  \"no\"  \"yes\" \"no\"  \"no\"  \"no\"  \"no\"  \"yes\" \"no\" \n[13] \"no\"  \"no\"  \"no\"  \"no\"  \"no\"  \"no\"  \"no\"  \"no\" \n\nhead(sample(real_estate$pool, size = length(real_estate$pool)), n = 20)\n\n [1] \"no\"  \"no\"  \"no\"  \"yes\" \"no\"  \"no\"  \"yes\" \"no\"  \"no\"  \"no\"  \"no\"  \"no\" \n[13] \"no\"  \"no\"  \"no\"  \"no\"  \"no\"  \"no\"  \"no\"  \"yes\"\n\n# create a function to do both steps, permute first then calculate difference\n# -&gt; this is different than the first example \ncompare_high_end_perm &lt;- function(df) {\n  \n  # sample without replacement to permute pool labels (keeping structure of dataset, just different order)\n  df$pool = sample(df$pool, size = length(df$pool))\n  \n  # calculate \n  with(df, quantile(price[pool == \"yes\"], 0.9) - quantile(price[pool == \"no\"], 0.9))\n\n}\n\n# initialize items and loop over iterations\nM &lt;- 10000\npool_diffs &lt;- rep(NA, M) \nfor (i in 1:M) {\n  # rerandomize and compute every iteration\n  # could have done both permuting and calculating inside the for loop as well, but cleaner with functions\n  pool_diffs[i] = compare_high_end_perm(real_estate)\n}\n\n# equivalent to, just depends on how want to read code\n# -&gt; ignore X entirely\npool_diffs2 &lt;- sapply(1:M, function(X) compare_high_end_perm(real_estate))\npool_diffs3 &lt;- purrr::map_dbl(1:M, function(X) compare_high_end_perm(real_estate))\n\n# now look at results and p-value\n# -&gt; upper-tailed test based on context and order of subtraction in function\nreal_diff &lt;- with(real_estate, quantile(price[pool == \"yes\"], 0.9) - quantile(price[pool == \"no\"], 0.9))\nmean(pool_diffs &gt;= real_diff)\n\n[1] 0.0899\n\n\nBased on this p-value, there is insufficient evidence to suggest that the 90th quantile of house prices with pools is greater than that without pools.\nCan use Monte Carlo simulation in lots of scenarios. For example in regression, if know what the null model is, can simulate lots of datasets and models and study the behaviors of different aspects of the model such as coefficients or \\(R^2\\) and compare that to what was observed with the real data.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "using-and-building-functions.html#overview",
    "href": "using-and-building-functions.html#overview",
    "title": "2  Using and Building Functions",
    "section": "\n2.1 Overview",
    "text": "2.1 Overview\nThis section will discuss R functions. Functions are the engines that define how to DO things with your data objects in R. If data objects are the “nouns”, then functions are the “verbs” of the R language.\nWhy build functions?\n\nBecause it is fun (for us nerdy people).\nBecause we find ourselves doing the same task multiple times throughout our code, and realize that each iteration is mostly the same process using slightly different values or variables.\nBecause we want to avoid copy/paste related errors.\nBecause we want to make our code more readable.\n\nThis will discuss the basic structure and components of a function, how a function is created, passing parameters implicitly versus explicitly in a function call, name masking in nested environments, and infix operators.\nBelow are links to supplementary resources about building functions in R:\n\nAdvanced R Functions Chapter\nAdditional Functions Reading from R for Data Science E-book",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using and Building Functions</span>"
    ]
  },
  {
    "objectID": "using-and-building-functions.html#r-function-basics---components",
    "href": "using-and-building-functions.html#r-function-basics---components",
    "title": "2  Using and Building Functions",
    "section": "\n2.2 R Function Basics - Components",
    "text": "2.2 R Function Basics - Components\n\n``To understand computations in R, two slogans are helpful: Everything that exists is an object. Everything that happens is a function call.’’ — John Chambers\n\nFormals: The defined parameters of a function. Think of these as the set of data objects and options that can be defined by the programmer when a function is used.\nFor these exercises we will consider the following functions:\n\n\nmean() from base R\n\nggplot() from the ggplot2 package\n\n%&gt;% piping symbol from the dplyr package\n\n\nlibrary(tidyverse)\n\n# a function without parentheses prints what it is\n# -&gt; with the parenthesis, it tells R to run it\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x7ff0ab392578&gt;\n&lt;environment: namespace:base&gt;\n\nggplot\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \n{\n    UseMethod(\"ggplot\")\n}\n&lt;bytecode: 0x7ff0bdbab968&gt;\n&lt;environment: namespace:ggplot2&gt;\n\n\nPipe\n\nThis is actually quite a complex function (yes, a function) (things like this are called infix operator). Infix operators go in between arguments.\nCan surround an infix operator with tick marks to see what this function is actually doing.\n\n\n# infix operator: pipe\n`%&gt;%`\n\nfunction (lhs, rhs) \n{\n    lhs &lt;- substitute(lhs)\n    rhs &lt;- substitute(rhs)\n    kind &lt;- 1L\n    env &lt;- parent.frame()\n    lazy &lt;- TRUE\n    .External2(magrittr_pipe)\n}\n&lt;bytecode: 0x7ff0ac2caeb0&gt;\n&lt;environment: namespace:magrittr&gt;\n\n\n\nAnother example is +. This is a primitive function, which just means that R doesn’t evaluate it. It sends it down and is evaluated in C (because it is more efficient).\n\n\n# infix operator as function\n`+`(2, 3)\n\n[1] 5\n\n\n\n2.2.1 Formals\nWhat are the formals for these functions?\n\n\n?formals(): Formals are things that we define to change the behavior of the outcome.\n\n\n# view formals\nformals(`%&gt;%`)\n\n$lhs\n\n\n$rhs\n\nformals(mean)\n\n$x\n\n\n$...\n\n\n\n... are a special (very flexible, catch all) type of formal argument that we can put non-necessary arguments.\nFor example, with mean mean(x, na.rm = TRUE), this saves the value TRUE to na.rm argument and saves that value as the function is run.\n\n2.2.2 Body\nThe lines of code that will be run when the function is called. The body uses the formals and usually returns a result of some kind.\n\n# body()\nbody(`+`)\n\nNULL\n\n# -&gt; returns null because the function isn't run in R\n\nbody(ggplot)\n\n{\n    UseMethod(\"ggplot\")\n}\n\n\n\nNote that the code in the body is never commented because R strips away the comments when running to make its more compact. When we write functions, we add comments so the humans know what it does, but when it is stored they are removed to take up less space.\n\n2.2.3 Environment\nThe environment is the group of named objects which functions have access to. In other words, it is where the function will retrieve named objects used to execute the code from the body.\n\nThe global environment is everything that we have loaded in, created ourselves or in base R.\nls() tells you what you have created / assigned in the global environment (to get, set, test for and create environments, use environment()).\n\n\n# ?environment()\nls()\n\n[1] \"has_annotations\"\n\ny &lt;- 1:5\nls()\n\n[1] \"has_annotations\" \"y\"              \n\n\n\nWhen calling functions, they have access to the names of objects in the namespace. It can map the name to where the object is stored. So it retrieves values through the names, so names are very powerful and why we can do computation work that is readable.\n\n\n# y is now in the namespace, so we have access to it\n# -&gt; the name gets mapped to its value\ny*2\n\n[1]  2  4  6  8 10\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# -&gt; we were able to find this because head and iris are a namespace in the global environment\n\n\nWhen we load a library, we are loading a namespace that has access to all the function names and dataset names in the library (package). Everything is a named object (objects, functions, etc.) and they are saved somewhere. The environment is where R finds the object.\n\nLocal namespaces\n\nWhen a function is called, a local namespace is created within the global environment (like a subspace of the global environment).\nWhen code is run in this local environment, R has access to all objects contained in the global environment.\nThe primary difference is that when the local namespace is created, the formal argument names and their defined values are written into the local namespace. If these objects already exist in the global environment, they are overwritten locally while the function code is running. This is referred to as name masking.\nAfter the function code has completed running, the local environment is deleted. Thus, the local environment of a function is both nested in the global environment and is temporary.\n\n\n# when we call the mean function, x is defined locally but not globally\nmean(x = y)\n\n[1] 3",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using and Building Functions</span>"
    ]
  },
  {
    "objectID": "using-and-building-functions.html#creating-functions",
    "href": "using-and-building-functions.html#creating-functions",
    "title": "2  Using and Building Functions",
    "section": "\n2.3 Creating Functions",
    "text": "2.3 Creating Functions\nTo create a function you must define the formals and body in using the function() function, and assign the definition to a name.\n\nBelow we are giving a function as a value to the function name.\nFunctions are first class objects (they store expressions, rather than data values).\nIf there is a set of things we want to do, we can wrap a bunch of code into an expression by wrapping in curly brackets, then R thinks about it as one thing.\n\n\n# writing functions\nfunction_name &lt;- function(formal_arguments_go_here) {\n  # code body expression goes here\n}\n\nformals(function_name)\n\n$formal_arguments_go_here\n\nbody(function_name)\n\n{\n}\n\nenvironment(function_name)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\nAlso note, function() is a very weird function, since the body argument goes outside the parentheses! Technically even the example above is a real function (albeit a boring one).\nBetter example of simple function for adding up two numbers.\n\nGive functions a name that indicate what it does (intuitive names help a great deal with readable code).\n\nStyle guide: Can use the assignment arrow &lt;- for anytime something will be saved to the global environment (data values and functions), but within a function we can use = because it is only found locally and is temporary.\n\n\n&lt;&lt;- Assigns values from a local environment (like a function call) and saves it in the global environment (this is a really bad idea because it could overwrite something that you already have in the global environment)\n\n\n\nReturning values: Functions implicitly return the last thing computed. So it is often a good idea to be explicit about what to return.\n\nAll standalone functions should have explicit returns. When doing an on-the-fly function in an sapply() for example, we can stylistically omit return() and still have readable code.\n\n\n\n\n# write function add two numbers\nmy_sum &lt;- function(x, y){\n  val = x + y\n  return(val)\n}\nmy_sum(1,2)\n\n[1] 3\n\n# bad idea\nmy_sum &lt;- function(x, y){\n  val &lt;&lt;- x + y\n  return(val)\n}\n# val isn't in the global environment\nmy_sum(1, 2)\n\n[1] 3\n\n\nMore complex functions: In a legitimate function, to have our function behave differently, we can use some if and else statements that are usually triggered by a yes / no or true / false.\nsapply() is a function that calls other functions in an iterative way (really efficient for loop).\n\n# label: defining-functions-ifs\n\n# now trying to do something cool (and / or breaking it)\n# -&gt; on rare occasions, add some random error to the sum\nmy_sum &lt;- function(x, y, mischief = TRUE){\n  if(mischief == FALSE){\n    val = x + y\n  } else {\n    val = x + y + sample(c(-1,0,1), size = 1, prob = c(.02,.96,.02))\n  }\n  return(val)\n}\n\n# run the function 100 times using specified values for the arguments\n# -&gt; not using the X in our function call\nsapply(X = 1:100, my_sum, x = 1, y = 2)\n\n  [1] 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n\n# equivalent to\nfor (i in 1:10) {\n  print(my_sum(1, 2))\n}\n\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 4\n[1] 3\n[1] 3\n[1] 3\n[1] 2\n[1] 3\n\n\nCan return something more complex than just a data value by returning lists. Functions that output lists are kinda the next step of complex computing ideas.\n\n# label: defining-functions-return-list\n\n# modify function to return a list with multiple objects in it\nmy_sum &lt;- function(x, y, mischief = TRUE){\n  if(mischief == FALSE){\n    val = x + y\n  } else {\n    val = x + y + sample(c(-1,0,1), size = 1, prob = c(.02,.96,.02))\n  }\n  return(list(value = val,\n              mischief = mischief))\n}\n\nmy_sum(1, 2)\n\n$value\n[1] 3\n\n$mischief\n[1] TRUE\n\n\n\n2.3.1 Calling a Function\nTo call a function, you simply use its name and define any required formal arguments. Typically this is done within a set of parentheses directly following the function name. Only a few special cases break from that pattern.\nWhat is more important to focus on is if and how we define the arguments. We may choose to define nothing in the arguments and the function might still work.\n\n# write function that has no arguments to simply return this hardcoded value\nno_arg &lt;- function(){\n  x = 10\n  return(x)\n}\nno_arg()\n\n[1] 10\n\n# -&gt; so it will generate an error if we tried to give it an argument that isn't defined in the function\nno_arg(x = 20)\n\nError in no_arg(x = 20): unused argument (x = 20)\n\n\nTypically these are made more flexible using dots .... All this does is put the values into the local namespace and continues running the function. These are optional arguments.\n\n# define function wtih ... formal\nno_arg2 &lt;- function(...){\n  x = 10\n  return(x)\n}\n\n# functions works with no arguments\nno_arg2() \n\n[1] 10\n\n# and now with arguments\n# -&gt; it initializes the value of x, but then  overwrites the value of x internally\nno_arg2(x = 20)\n\n[1] 10\n\n\nFor “optional” arguments (arguments that sometimes matters and sometimes don’t matter), we should define them as formal parameters with some default values. This way we can override them if we wanted to.\n\n# add default value to argument\n# -&gt; now it behaves typically how we want, but can change them if we want\nno_arg3 &lt;- function(x = 10){\n  return(x)\n}\n\n# this is the behavior we typically will want\nno_arg3()\n\n[1] 10\n\nno_arg3(x = 20)\n\n[1] 20\n\n\nThe more important characteristics of function calls are related to how we define our arguments: implicitly and explicitly. If we rely on the default ordering of the arguements, then we are calling the arguments implicitly. If we refer to the argument by it’s name while defining the values, then we are calling the argument explicitly. Often we use a mix of these methods, but it is important to be aware of how and why we choose to define our arguments.\nA function runs through all of the named arguments, then goes in order for the unnamed arguments.\n\n# all of the following are equivalent\nmean(x = 1:5, na.rm = TRUE)\n\n[1] 3\n\nmean(1:5, na.rm = TRUE)\n\n[1] 3\n\nmean(na.rm = TRUE, x = 1:5)\n\n[1] 3\n\n# the following is NOT equivalent\n# -&gt; it is expecting a numeric first\nmean(TRUE, 1:5)\n\nError in mean.default(TRUE, 1:5): 'trim' must be numeric of length one\n\n\nPossible convention: Implicitly give the function data values first, then use explicit calling for the options (this will still be readable because the function name will indicate what it’s going to do with the data values).\n\n2.3.2 Automating procedures\nThe following example demonstrates code that would be better served by constructing a function to accomplish the task.\n\n# take the following olive oil data and standardize the columns\nlibrary(pdfCluster) # for the data\ndata(oliveoil) # ?oliveoil\nhead(oliveoil)\n\n  macro.area       region palmitic palmitoleic stearic oleic linoleic linolenic\n1      South Apulia.north     1075          75     226  7823      672        36\n2      South Apulia.north     1088          73     224  7709      781        31\n3      South Apulia.north      911          54     246  8113      549        31\n4      South Apulia.north      966          57     240  7952      619        50\n5      South Apulia.north     1051          67     259  7771      672        50\n6      South Apulia.north      911          49     268  7924      678        51\n  arachidic eicosenoic\n1        60         29\n2        61         29\n3        63         29\n4        78         35\n5        80         46\n6        70         44\n\n# start with a dataframe where create the z-scores for palmitic acid, then add the rest \noo_standardized &lt;- data.frame(\n  palmitic = (oliveoil[,\"palmitic\"]-mean(oliveoil[,\"palmitic\"]))/sd(oliveoil[,\"palmitic\"])\n)\nhead(oo_standardized)\n\n    palmitic\n1 -0.9297061\n2 -0.8525970\n3 -1.9024672\n4 -1.5762364\n5 -1.0720614\n6 -1.9024672\n\n# if wanted to do this manually for all columns we would do this\noo_standardized &lt;- data.frame(\n  palmitic = (oliveoil[,\"palmitic\"]-mean(oliveoil[,\"palmitic\"]))/sd(oliveoil[,\"palmitic\"]),\n  palmitoleic = (oliveoil[,\"palmitoleic\"]-mean(oliveoil[,\"palmitoleic\"]))/sd(oliveoil[,\"palmitoleic\"]),\n  stearic = (oliveoil[,\"stearic\"]-mean(oliveoil[,\"stearic\"]))/sd(oliveoil[,\"stearic\"]),\n  oleic = (oliveoil[,\"oleic\"]-mean(oliveoil[,\"oleic\"]))/sd(oliveoil[,\"oleic\"]),\n  linoleic = (oliveoil[,\"linoleic\"]-mean(oliveoil[,\"linoleic\"]))/sd(oliveoil[,\"linoleic\"]),\n  linolenic = (oliveoil[,\"linolenic\"]-mean(oliveoil[,\"linolenic\"]))/sd(oliveoil[,\"linolenic\"]),\n  arachidic = (oliveoil[,\"arachidic\"]-mean(oliveoil[,\"arachidic\"]))/sd(oliveoil[,\"arachidic\"]),\n  eicosenoic = (oliveoil[,\"eicosenoic\"]-mean(oliveoil[,\"eicosenoic\"]))/sd(oliveoil[,\"eicosenoic\"])\n)\nhead(oo_standardized)\n\n    palmitic palmitoleic    stearic     oleic   linoleic  linolenic  arachidic\n1 -0.9297061  -0.9733313 -0.0779804 1.2598296 -1.2707124  0.3170625 0.08634028\n2 -0.8525970  -1.0114306 -0.1324097 0.9789102 -0.8217818 -0.0684812 0.13173241\n3 -1.9024672  -1.3733742  0.4663123 1.9744494 -1.7773038 -0.0684812 0.22251667\n4 -1.5762364  -1.3162252  0.3030245 1.5777122 -1.4889997  1.3965850 0.90339865\n5 -1.0720614  -1.1257286  0.8201026 1.1316909 -1.2707124  1.3965850 0.99418291\n6 -1.9024672  -1.4686225  1.0650343 1.5087145 -1.2460006  1.4736938 0.54026160\n  eicosenoic\n1  0.9030934\n2  0.9030934\n3  0.9030934\n4  1.3291301\n5  2.1101973\n6  1.9681851\n\n\nCould do this (slighlty) better with a function, rather than copy and paste and change column names.\n\n# write function to take a variable name and return the standardized column\noo_stand &lt;- function(varname) {\n  (oliveoil[,varname]-mean(oliveoil[,varname]))/sd(oliveoil[,varname])\n}\noo_stand(\"palmitic\")[1:10]\n\n [1] -0.9297061 -0.8525970 -1.9024672 -1.5762364 -1.0720614 -1.9024672\n [7] -1.8372211 -0.7814194 -0.8881858 -1.1551020\n\n# building the standardized data almost as repetitive as previous approach\noo_standardized2 &lt;- data.frame(\n  palmitic =  oo_stand(\"palmitic\"),\n  palmitoleic =  oo_stand(\"palmitoleic\"),\n  stearic =  oo_stand(\"stearic\"),\n  oleic =  oo_stand(\"oleic\"),\n  linoleic =  oo_stand(\"linoleic\"),\n  linolenic =  oo_stand(\"linolenic\"),\n  arachidic =  oo_stand(\"arachidic\"),\n  eicosenoic =  oo_stand(\"eicosenoic\")\n)\nhead(oo_standardized2)\n\n    palmitic palmitoleic    stearic     oleic   linoleic  linolenic  arachidic\n1 -0.9297061  -0.9733313 -0.0779804 1.2598296 -1.2707124  0.3170625 0.08634028\n2 -0.8525970  -1.0114306 -0.1324097 0.9789102 -0.8217818 -0.0684812 0.13173241\n3 -1.9024672  -1.3733742  0.4663123 1.9744494 -1.7773038 -0.0684812 0.22251667\n4 -1.5762364  -1.3162252  0.3030245 1.5777122 -1.4889997  1.3965850 0.90339865\n5 -1.0720614  -1.1257286  0.8201026 1.1316909 -1.2707124  1.3965850 0.99418291\n6 -1.9024672  -1.4686225  1.0650343 1.5087145 -1.2460006  1.4736938 0.54026160\n  eicosenoic\n1  0.9030934\n2  0.9030934\n3  0.9030934\n4  1.3291301\n5  2.1101973\n6  1.9681851\n\n\nNext, can do this iteratively with for-loops to first print the result (for demonstration) and then to store the result.\n\n# intitialize names\nacids &lt;- names(oliveoil[3:10])\n\n# loop over names and print first few standardized observations\nfor (acid_name in acids){\n  print(oo_stand(acid_name)[1:5])\n}\n\n[1] -0.9297061 -0.8525970 -1.9024672 -1.5762364 -1.0720614\n[1] -0.9733313 -1.0114306 -1.3733742 -1.3162252 -1.1257286\n[1] -0.0779804 -0.1324097  0.4663123  0.3030245  0.8201026\n[1] 1.2598296 0.9789102 1.9744494 1.5777122 1.1316909\n[1] -1.2707124 -0.8217818 -1.7773038 -1.4889997 -1.2707124\n[1]  0.3170625 -0.0684812 -0.0684812  1.3965850  1.3965850\n[1] 0.08634028 0.13173241 0.22251667 0.90339865 0.99418291\n[1] 0.9030934 0.9030934 0.9030934 1.3291301 2.1101973\n\n# initialization storage space for transformed values\noo_standardized3 &lt;- matrix(data = rep(NA, nrow(oliveoil) * 8), \n                           nrow = nrow(oliveoil)) %&gt;% \n  as.data.frame\nnames(oo_standardized3) &lt;- names(oliveoil)[3:10]\n\n# loop over the acid names\nfor (acid_name in acids){\n  oo_standardized3[,acid_name] &lt;- oo_stand(acid_name)\n}\n# check the outcome\nhead(oo_standardized3)\n\n    palmitic palmitoleic    stearic     oleic   linoleic  linolenic  arachidic\n1 -0.9297061  -0.9733313 -0.0779804 1.2598296 -1.2707124  0.3170625 0.08634028\n2 -0.8525970  -1.0114306 -0.1324097 0.9789102 -0.8217818 -0.0684812 0.13173241\n3 -1.9024672  -1.3733742  0.4663123 1.9744494 -1.7773038 -0.0684812 0.22251667\n4 -1.5762364  -1.3162252  0.3030245 1.5777122 -1.4889997  1.3965850 0.90339865\n5 -1.0720614  -1.1257286  0.8201026 1.1316909 -1.2707124  1.3965850 0.99418291\n6 -1.9024672  -1.4686225  1.0650343 1.5087145 -1.2460006  1.4736938 0.54026160\n  eicosenoic\n1  0.9030934\n2  0.9030934\n3  0.9030934\n4  1.3291301\n5  2.1101973\n6  1.9681851\n\n\nBest solution is to vectorize the procedure and use the APPLY type statements.\n\napply() used to apply functions over the indices of an array.\nlapply() used to apply functions over the values in a list, outputs to a list.\n\nsapply() used to apply functions over the values in a list, outputs to a simplified array.\n\n\nlapply() and sapply(simplify = FALSE) are the same thing, kinda how paste() and paste0() are the same except paste0() has a default separator (which is very useful, so they just gave it a shorthand function).\n\n\nvapply() used to apply functions over the values in a list, outputs to specified object type.\nmapply() used to apply functions over the corresponding values in multiple lists.\n\nKarsten’s Personal Use (with orders of magnitude &gt;): sapply \\(&gt;\\) lapply \\(&gt;&gt;\\) mapply \\(&gt;&gt;\\) vapply \\(&gt;&gt;\\) apply\n\n# loop over all column names with sapply()\n# -&gt; typically in practice the formals are given implicitly and just know that the object to be looped over is first and the function being applied is second\noo_better &lt;- sapply(X = acids, FUN = oo_stand) %&gt;% as.data.frame\nhead(better_oo)\n\nError in eval(expr, envir, enclos): object 'better_oo' not found\n\n# same thing, but return object is a list\noo_better_list &lt;- lapply(X = acids, FUN = oo_stand)\nstr(oo_better_list)\n\nList of 8\n $ : num [1:572] -0.93 -0.853 -1.902 -1.576 -1.072 ...\n $ : num [1:572] -0.973 -1.011 -1.373 -1.316 -1.126 ...\n $ : num [1:572] -0.078 -0.132 0.466 0.303 0.82 ...\n $ : num [1:572] 1.26 0.979 1.974 1.578 1.132 ...\n $ : num [1:572] -1.271 -0.822 -1.777 -1.489 -1.271 ...\n $ : num [1:572] 0.3171 -0.0685 -0.0685 1.3966 1.3966 ...\n $ : num [1:572] 0.0863 0.1317 0.2225 0.9034 0.9942 ...\n $ : num [1:572] 0.903 0.903 0.903 1.329 2.11 ...\n\n\nCan use tidyverse equivalents from the purrr package (documentation), specifically map().\n\nmap() always returns a list, so this would be equivalent lapply().\n\nOther variants include:\n\nmap_vec() must return a single value from each operation. If you know the output type, it is more efficient to use the corresponding variant such as map_dbl(), map_chr(), etc.\nindexed map imap() applies a function to each element of a vector, and its index (which is shorthand for a particular use of map2() which iterates over 2 arguments at a time).\nlmap() applies a function to list-elements of a list, useful for applying functions that take lists as arguments.\npmap() extends map2() to iterate over multiple arguments simultaneously.\n\n\n\n\n# loop over all column names with sapply()\nlibrary(purrr)\n#  \n# help(package=\"purrr\")\n# ?map\n\n# apply same function over names of columns using purrr equivalent function to lapply\noo_purrr_list &lt;- map(.x = acids, .f = oo_stand)\nstr(oo_purrr_list)\n\nList of 8\n $ : num [1:572] -0.93 -0.853 -1.902 -1.576 -1.072 ...\n $ : num [1:572] -0.973 -1.011 -1.373 -1.316 -1.126 ...\n $ : num [1:572] -0.078 -0.132 0.466 0.303 0.82 ...\n $ : num [1:572] 1.26 0.979 1.974 1.578 1.132 ...\n $ : num [1:572] -1.271 -0.822 -1.777 -1.489 -1.271 ...\n $ : num [1:572] 0.3171 -0.0685 -0.0685 1.3966 1.3966 ...\n $ : num [1:572] 0.0863 0.1317 0.2225 0.9034 0.9942 ...\n $ : num [1:572] 0.903 0.903 0.903 1.329 2.11 ...\n\n# same thing, but now simplify the result\nmap_vec(.x = acids, .f = oo_stand)\n\nError in `map_vec()`:\n! `out[[1]]` must have size 1, not size 572.\n\n# modify function to just return the st dev of the standardized columns\noo_stand_sd &lt;- function(varname) {\n  sd((oliveoil[,varname]-mean(oliveoil[,varname]))/sd(oliveoil[,varname]))\n}\nmap_vec(.x = acids, .f = oo_stand_sd)\n\n[1] 1 1 1 1 1 1 1 1\n\n# know returning a double, so use map_dbl()\nmap_dbl(.x = acids, .f = oo_stand_sd)\n\n[1] 1 1 1 1 1 1 1 1\n\n\nMini timing study to see which method of looping is most efficient.\n\nAPPLY statements are quicker than for because they are run in C rather than R.\n\n\n# create lots of lm from different random samples and store the slope of a particular variable\nn &lt;- 1000\nbetas &lt;- rep(NA, n)\n\n# initialize timing study and use for loop\ntimer_for &lt;- Sys.time()\nfor(i in 1:n){\n  betas[i] &lt;- lm(palmitic ~ . , data = slice_sample(oliveoil, n = 100))$coefficients[\"oleic\"]\n}\nSys.time() - timer_for\n\nTime difference of 1.51896 secs\n\n# now use sapply()\n# -&gt; again not really using X = 1:n in the function that sapply() calls, it is just for the iterations to do the thing that many times\nbetas &lt;- rep(NA, n)\ntimer_sapply &lt;- Sys.time()\nbetas &lt;- sapply(1:n, function(x) {\n    lm(palmitic ~ . , data = slice_sample(oliveoil, n = 100))$coefficients[\"oleic\"]\n})\nSys.time() - timer_sapply\n\nTime difference of 1.474225 secs\n\n# now use map()\nbetas &lt;- rep(NA, n)\ntimer_map &lt;- Sys.time()\nbetas &lt;- map(1:n, \\(x) lm(palmitic ~ . , data = slice_sample(oliveoil, n = 100))$coefficients[\"oleic\"])\nSys.time() - timer_map\n\nTime difference of 1.468056 secs\n\n# now use map_dbl()\nbetas &lt;- rep(NA, n)\ntimer_dbl &lt;- Sys.time()\nbetas &lt;- map_dbl(1:n, \\(x) lm(palmitic ~ . , data = slice_sample(oliveoil, n = 100))$coefficients[\"oleic\"])\nSys.time() - timer_dbl\n\nTime difference of 1.503587 secs",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using and Building Functions</span>"
    ]
  },
  {
    "objectID": "using-and-building-functions.html#applying-functions-over-multiple-sets-of-parameters",
    "href": "using-and-building-functions.html#applying-functions-over-multiple-sets-of-parameters",
    "title": "2  Using and Building Functions",
    "section": "\n2.4 Applying functions over multiple sets of parameters",
    "text": "2.4 Applying functions over multiple sets of parameters\n\n2.4.1 Simulate data\nIn some cases we may wish to apply a complex function by putting in multiple sets of arguments and collecting the results. In the example below we explore a simple case where we want to gather simulated values from a beta distribution with several different shape parameter (\\(\\alpha\\), \\(\\beta\\)) pairs.\n\n# let's check what the rbeta() function does first in the help menu\n# ?rbeta\n# simulate n observations from a specific beta distribution\nrbeta(5, shape1 = 1, shape2 = 1)\n\n[1] 0.4420070 0.6736804 0.2605573 0.5639032 0.9110985\n\n# want to apply the rbeta() function over all 100 unique combinations\n# of shape1 = {0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}\n# and shape2 = {0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}\nparam_combinations &lt;- expand.grid(shape1 = seq(0.5, 5, by = 0.5),\n                                  shape2 = seq(0.5, 5, by = 0.5))\nhead(param_combinations)\n\n  shape1 shape2\n1    0.5    0.5\n2    1.0    0.5\n3    1.5    0.5\n4    2.0    0.5\n5    2.5    0.5\n6    3.0    0.5\n\n\nCould create two nested for loops to iterate over these pairs and store as it loops, but that is messy. Instead we will iterate over each pair using mapply() and map2().\n\n# run simulation to generate 5 values beta values from each parameter combo\n# use mapply() and simplify the result\nset.seed(12345)\nmy_beta_sims &lt;- mapply(FUN = rbeta, \n                       shape1 = param_combinations$shape1,\n                       shape2 = param_combinations$shape2,\n                       MoreArgs = list(n = 5),\n                       SIMPLIFY = \"array\")\n\n# map way shown at end\n\n# rows in parameter storage correspond to the columns in the simulation array\nparam_combinations[1:10,]\n\n   shape1 shape2\n1     0.5    0.5\n2     1.0    0.5\n3     1.5    0.5\n4     2.0    0.5\n5     2.5    0.5\n6     3.0    0.5\n7     3.5    0.5\n8     4.0    0.5\n9     4.5    0.5\n10    5.0    0.5\n\nmy_beta_sims[,1:10]\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 0.5863837 0.4459386 0.9777934 0.4583129 0.9999148 0.9963217 0.8925415\n[2,] 0.8116711 0.5797895 0.5087671 0.9677340 0.9994157 0.9926236 0.9558317\n[3,] 0.9987221 0.8680033 0.8865398 0.9668572 0.9606193 0.9901395 0.4404319\n[4,] 0.1143234 0.9148146 0.9748052 0.7042567 0.6387993 0.9741483 0.9445973\n[5,] 0.7077567 0.1463414 0.2696026 0.9829616 0.8444588 0.9754417 0.9852225\n          [,8]      [,9]     [,10]\n[1,] 0.9280662 0.9951651 0.9999956\n[2,] 0.9995616 0.9927259 0.9960698\n[3,] 0.9770016 0.9766352 0.9998980\n[4,] 0.9049755 0.9865606 0.9889205\n[5,] 0.9990467 0.7723663 0.7884314\n\n# use map2()\n# -&gt; returns a list, so have to reduce it to turn into a matrix and then set the colnames to be readable\nmy_beta_sims &lt;- param_combinations %&gt;% \n  {map2(.$shape1, .$shape2,\n       \\(shape1, shape2) rbeta(n = 5, shape1 = shape1, shape2 = shape2))} %&gt;% \n  reduce(cbind)\ncolnames(my_beta_sims) &lt;- paste(\"sim\", 1:nrow(param_combinations))\n\nOr even better organization is to turn this into an 3-dimensional array where the index [i,j,k] represent the \\(i\\)th simulation, \\(j\\)th shape1, and \\(k\\)-th shape2.\n\n# convert copy of matrix to array (remember )\nmy_beta_sims_array &lt;- my_beta_sims\ndim(my_beta_sims_array) &lt;- c(5,10,10)\n\n# remember R goes down the columns first to fill the first layer\n# -&gt; then starts the next layer down the columns....\nmy_beta_sims_array[ , , 1] == my_beta_sims[, 1:10]\n\n     sim 1 sim 2 sim 3 sim 4 sim 5 sim 6 sim 7 sim 8 sim 9 sim 10\n[1,]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE   TRUE\n[2,]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE   TRUE\n[3,]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE   TRUE\n[4,]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE   TRUE\n[5,]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE   TRUE\n\n# display all simulations from the first shape 2 (columns = simulation and rows = iteration)\nmy_beta_sims_array[ , , 1]\n\n            [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 0.006280542 0.7532718 0.4820312 0.9935087 0.8546249 0.9423096 0.6818702\n[2,] 0.341378768 0.6908183 0.9369698 0.1185105 0.9269818 0.9353278 0.9671111\n[3,] 0.544664311 0.8313604 0.8681536 0.7087383 0.9975304 0.6857775 0.9221920\n[4,] 0.888962458 0.5024863 0.6659881 0.9869572 0.9667159 0.6357080 0.8850950\n[5,] 0.551292466 0.9969978 0.9872116 0.8641136 0.7538840 0.9777792 0.9969331\n          [,8]      [,9]     [,10]\n[1,] 0.9598644 0.9993042 0.8993634\n[2,] 0.7964658 0.9504420 0.7584427\n[3,] 0.8090599 0.9972686 0.9782978\n[4,] 0.9996361 0.9822641 0.8401233\n[5,] 0.7822795 0.8821006 0.9076706\n\n\n\n2.4.2 Summarize simulation\nNow suppose we want the mean for the simulation outcomes array. Use apply() to iterate over indices of the array.\n\n# calc mean of a particular iteration\nmean(my_beta_sims_array[ ,1 , 1])\n\n[1] 0.4665157\n\n# loop over each column in each layer (thus averaging over the rows) with apply()\nbeta_means &lt;- apply(X = my_beta_sims_array,\n                    MARGIN = c(2,3),\n                    FUN = mean)\nstr(beta_means)\n\n num [1:10, 1:10] 0.467 0.755 0.788 0.734 0.9 ...\n\n# map way requires different functions\n# -&gt; need to convert the array to a list first with array_tree() (a hierarchical list), and then map at the second level\n# -&gt; then simplify\n# NOTE -&gt; changing the order of dimensions in MARGIN transposes the result (i.e. order matters)\nbeta_means2 &lt;- my_beta_sims_array %&gt;% \n  array_tree(margin = c(3, 2)) %&gt;% # first level list will be the layers, then the columns result$layer[[column]]\n  map_depth(.depth = 2, .f = mean) %&gt;% \n  reduce(cbind)\nrownames(beta_means2) &lt;- paste0(\"shape1_\", unique(param_combinations$shape1))\ncolnames(beta_means2) &lt;- paste0(\"shape2_\", unique(param_combinations$shape2))\n\n# compare two different methods\nbeta_means2 == beta_means\n\n           shape2_0.5 shape2_1 shape2_1.5 shape2_2 shape2_2.5 shape2_3\nshape1_0.5       TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_1         TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_1.5       TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_2         TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_2.5       TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_3         TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_3.5       TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_4         TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_4.5       TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\nshape1_5         TRUE     TRUE       TRUE     TRUE       TRUE     TRUE\n           shape2_3.5 shape2_4 shape2_4.5 shape2_5\nshape1_0.5       TRUE     TRUE       TRUE     TRUE\nshape1_1         TRUE     TRUE       TRUE     TRUE\nshape1_1.5       TRUE     TRUE       TRUE     TRUE\nshape1_2         TRUE     TRUE       TRUE     TRUE\nshape1_2.5       TRUE     TRUE       TRUE     TRUE\nshape1_3         TRUE     TRUE       TRUE     TRUE\nshape1_3.5       TRUE     TRUE       TRUE     TRUE\nshape1_4         TRUE     TRUE       TRUE     TRUE\nshape1_4.5       TRUE     TRUE       TRUE     TRUE\nshape1_5         TRUE     TRUE       TRUE     TRUE\n\n\nNow we can check our simulated means against a theoretical mean: \\(X \\sim \\text{beta}(\\alpha, \\beta) \\Longrightarrow E(X) = \\frac{\\alpha}{\\alpha + \\beta}\\).\n\n# define function to calculate theoretical beta mean\ntheoretical_beta_mean &lt;- function(alpha, beta){\n  alpha / (alpha + beta)\n}\n\n# create a 10 x 10 matrix of the theoretical means for the beta distribution using the function above\ntheoretical_means &lt;- mapply(FUN = theoretical_beta_mean,\n                            alpha = param_combinations$shape1,\n                            beta = param_combinations$shape2,\n                            SIMPLIFY = \"array\")\ndim(theoretical_means) &lt;- c(10, 10)\ntheoretical_means\n\n           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n [1,] 0.5000000 0.3333333 0.2500000 0.2000000 0.1666667 0.1428571 0.1250000\n [2,] 0.6666667 0.5000000 0.4000000 0.3333333 0.2857143 0.2500000 0.2222222\n [3,] 0.7500000 0.6000000 0.5000000 0.4285714 0.3750000 0.3333333 0.3000000\n [4,] 0.8000000 0.6666667 0.5714286 0.5000000 0.4444444 0.4000000 0.3636364\n [5,] 0.8333333 0.7142857 0.6250000 0.5555556 0.5000000 0.4545455 0.4166667\n [6,] 0.8571429 0.7500000 0.6666667 0.6000000 0.5454545 0.5000000 0.4615385\n [7,] 0.8750000 0.7777778 0.7000000 0.6363636 0.5833333 0.5384615 0.5000000\n [8,] 0.8888889 0.8000000 0.7272727 0.6666667 0.6153846 0.5714286 0.5333333\n [9,] 0.9000000 0.8181818 0.7500000 0.6923077 0.6428571 0.6000000 0.5625000\n[10,] 0.9090909 0.8333333 0.7692308 0.7142857 0.6666667 0.6250000 0.5882353\n           [,8]      [,9]      [,10]\n [1,] 0.1111111 0.1000000 0.09090909\n [2,] 0.2000000 0.1818182 0.16666667\n [3,] 0.2727273 0.2500000 0.23076923\n [4,] 0.3333333 0.3076923 0.28571429\n [5,] 0.3846154 0.3571429 0.33333333\n [6,] 0.4285714 0.4000000 0.37500000\n [7,] 0.4666667 0.4375000 0.41176471\n [8,] 0.5000000 0.4705882 0.44444444\n [9,] 0.5294118 0.5000000 0.47368421\n[10,] 0.5555556 0.5263158 0.50000000\n\n# now use map2, then simplify to vector and convert to matrix\ntheoretical_means2 &lt;- param_combinations %&gt;% \n  {map2(.$shape1, .$shape2, \\(alpha, beta) theoretical_beta_mean(alpha, beta))} %&gt;% \n  as_vector\ndim(theoretical_means2) &lt;- c(10, 10)\n\ntheoretical_means == theoretical_means2\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n [2,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n [3,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n [4,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n [5,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n [6,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n [7,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n [8,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n [9,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n[10,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  TRUE\n\n# compare a few means\n# -&gt; note that for some reason beta_means2 is stored as a list but displaying like a matrix? but it is correct\ntheoretical_means[2,7] - beta_means2[2,7][[1]]\n\n[1] -0.04582796\n\ntheoretical_means[1,10] - beta_means2[1,10][[1]]\n\n[1] 0.04812996\n\n\n\n2.4.3 Entire workflow\nTo confirm the simulated data is correct, we can do a bigger simulation (more iterations) and then compare to the theoretical means.\n\n# steps of simulation\n# -&gt; simulate data based on parameter combinations\n# -&gt; keep in list form where each list all the iterations for a parameter pair\n# -&gt; then unpack and convert to 10 x 10 matrix\nbeta_simulation &lt;- param_combinations %&gt;% \n  {map2(.$shape1, .$shape2, \\(shape1, shape2) rbeta(n = 1000, shape1 = shape1, shape2 = shape2))} %&gt;% \n  map(mean) %&gt;% \n  reduce(rbind)\ndim(beta_simulation) &lt;- c(10,10)\n\n# compare simulated and theoretical means\ntheoretical_means - beta_simulation\n\n               [,1]         [,2]          [,3]          [,4]          [,5]\n [1,] -0.0142132012  0.003116614  0.0066012052 -0.0007622041  0.0062963343\n [2,]  0.0100067763  0.013270507 -0.0080331498  0.0123747272 -0.0046966411\n [3,]  0.0013483645 -0.007303189  0.0002846672  0.0021937513 -0.0023641170\n [4,] -0.0049458229  0.001847125  0.0040680799 -0.0040171170 -0.0089856956\n [5,] -0.0030957859  0.007595387 -0.0110157358  0.0018415398  0.0025003515\n [6,] -0.0043906746 -0.016080434  0.0009762396  0.0004213263  0.0014271047\n [7,]  0.0021821881  0.000129152  0.0180482395 -0.0047434431  0.0090941503\n [8,]  0.0008452475 -0.003486720  0.0009930790  0.0001847752 -0.0012385592\n [9,]  0.0033797827 -0.008557892 -0.0031519925 -0.0061728992 -0.0003760041\n[10,]  0.0069978255  0.005697019 -0.0056672028 -0.0033885516  0.0038704320\n               [,6]          [,7]          [,8]          [,9]         [,10]\n [1,]  3.054469e-03 -0.0015928837  0.0052464166 -0.0032251524 -0.0005184324\n [2,] -4.201410e-03  0.0030931652 -0.0054156716 -0.0026176455  0.0022975290\n [3,] -2.646566e-03  0.0052396896  0.0015635734  0.0088116515  0.0036147661\n [4,]  1.740050e-03 -0.0013558452  0.0071321635 -0.0066822756 -0.0021917885\n [5,]  3.218331e-03 -0.0025266934  0.0006006874  0.0006198128 -0.0031791894\n [6,] -5.832885e-03  0.0006288153 -0.0005570289 -0.0002017058  0.0006356175\n [7,] -1.869705e-03  0.0001812270 -0.0017629098  0.0002394587  0.0002452288\n [8,] -2.764513e-03  0.0054854862  0.0025546891 -0.0006957005  0.0014663981\n [9,]  2.794567e-03 -0.0044905333  0.0013261447 -0.0038680504  0.0025212857\n[10,]  5.620049e-05  0.0051221631  0.0044932398 -0.0037773398 -0.0024546055",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using and Building Functions</span>"
    ]
  },
  {
    "objectID": "using-and-building-functions.html#writing-modular-functions",
    "href": "using-and-building-functions.html#writing-modular-functions",
    "title": "2  Using and Building Functions",
    "section": "\n2.5 Writing Modular Functions",
    "text": "2.5 Writing Modular Functions\nWhen you are writing functions to do a complex task, try to identify the system of simple tasks that are needed to accomplish it step-by-step. For example we might consider building functions to help us to automate the process of running stepwise selection based on AIC, then outputting a model object that we can use for prediction, or inference.\nWe might initially think of one big function that can take a model matrix, X, and does a bunch of linear algebra to get beta estimates, calculates an AIC for the fitted model, then removes a columns from X to reduce the model… and repeat… a lot. Is that what we find inside the step() function? Run body(step) to see.\nIt isn’t doing any of the linear algebra and AIC calculations we expected! The body of the step function is filled with, well more functions! It is function pinata!\nWhat we find is that the step() function calls many other functions to do the individual smaller tasks that make up the complex procedure. If we search closely, there is a function for dropping a row, drop1(), within a while loop that keeps dropping rows until a condition is met. There is a function for updating the fit of the model, update(), and calculating the AIC, extractAIC(). There is even a function created at the start of the body that are used to help organize and return a helpful print statement about the stepwise procedure: step.results().\nThese functions are often times called “helper functions”, completing the sub-tasks of the primary function. This modular format allows each function to complete a single simple task, and when used in combination, achieve a complex task.\nThus, when modularizing code, the end goal to have one “overall” function that just has some helper functions within. On the way to having modularized code, is it better to start with an overall function and add modules sequentially (so e.g. if we call the overall, it will call module 1, so we check if that works; then add module 2 and call the overall which runs mod 1 and mod 2)…. etc.\nBelow is an example (NOT RUN) to demostrate how helper functions can be used so that the overall function is body is relatively simple, all the work is being done by other functions. The overall function should be readable step by step.\n\n# main data simulation function\n# write a function to simulate a dataset for an n x p dataset for a regression model where q of the variables are inactive (signal variables)\nmake_sim_data &lt;- function(n = 100, p = 20, q = 10, b = 0.1, sd_y = 1, sd_x = 1){\n  \n  # generate a n(iteration in sample) x p(sample) matrix from rnorm(0, sd_X)\n  X = sapply(1:p, function(i) rnorm(n, 0, sd = sd_x))\n  colnames(X) &lt;- paste0(\"x\",1:p)\n  \n  # specify population coefficients\n  betas = c(rep(b, q), rep(0, p-q))\n  \n  # generate responses\n  y = (X %*% betas)[,1] + rnorm(n,0, sd_y)\n  \n  # combine data\n  sim_data &lt;- data.frame(y,X)\n  \n  return(sim_data)\n\n}\n\n# helper functions for running the experiment with variable selection\n\n# function for choosing with stepwise and fitting a regression\nstep_var_mod &lt;- function(df){\n  step_selected &lt;- step(lm(y ~ . , data = df), trace = FALSE)\n  return(step_selected)\n}\n\n# function for choosing with lasso and fitting regression\nlasso_var_mod &lt;- function(df){\n  \n  cv.out = cv.glmnet(x = as.matrix(x = df[ ,-which(names(df) == \"y\")]),\n                      y = df$y, alpha = 1, type.measure = \"deviance\")\n  \n  lasso_mod = glmnet(x = as.matrix(x = df[, -which(names(df) == \"y\")]),\n                      y = df$y, alpha=1, lambda = cv.out$lambda.1se)\n  \n  lasso_vars = names(lasso_mod$beta[,1])[which(lasso_mod$beta[,1] != 0)]\n  \n  if (length(lasso_vars) == 0) lasso_vars &lt;- names(lasso_mod$beta[,1])[1]\n  \n  lasso_selected = lm(formula(paste0(\"y ~ 1 + \", paste(lasso_vars, collapse = \" + \"))), data = df)\n  \n  return(lasso_selected)\n  \n}\n\n# function for finding number of variables included\nselect_var_count &lt;- function(lin_mod){\n  length(coef(lin_mod))-1\n}\n\n# function for finding 10-fold cross validated RMSE\nselect_cv_rmse &lt;- function(lin_mod){\n  \n  cv_result = train(formula(lin_mod), \n    data = lin_mod$model,\n    method = \"lm\",\n    trControl = caret::trainControl(method = \"cv\", number = 10),\n    tuneGrid = data.frame(intercept=TRUE))\n  \n  return(cv_result$results$RMSE)\n  \n}\n\n# function to run trial \nrun_trial &lt;- function(selection_alg, df){\n  \n  start_time = Sys.time()\n  \n  tmp_mod = selection_alg(df) # not sure where this function comes from\n  \n  end_time = Sys.time()\n  \n  return(data.frame(var_count = select_var_count(tmp_mod),\n                    rmse = select_cv_rmse(tmp_mod), # nor this function\n                    time = difftime(end_time, start_time, units = \"secs\")))\n  \n}\n\n# make into a function of n_sims, n, p and q\nsim_var_select &lt;- function(n_sim = 10, n = 100, p = 10, q = 5, var_select_ftn = step_var_mod){\n  \n  results = NULL\n  \n  for(i in 1:n_sim){\n    \n    sim_data &lt;- make_sim_data(n = n, p = p,q = q)\n    \n    results &lt;- rbind(results, run_trial(var_select_ftn, sim_data))\n    \n  }\n  \n  return(results)\n}",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using and Building Functions</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Methods in R",
    "section": "",
    "text": "Overview\n&lt; insert table with links to each chapters and brief descriptions from the schedule &gt;\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/climate-dataviz.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "course-notes.html",
    "href": "course-notes.html",
    "title": "Course Notes",
    "section": "",
    "text": "This section contains the notes from the course.",
    "crumbs": [
      "Course Notes"
    ]
  },
  {
    "objectID": "simulation-studies.html#overview-of-computational-experiments-with-simulated-data",
    "href": "simulation-studies.html#overview-of-computational-experiments-with-simulated-data",
    "title": "6  Simulation Studies",
    "section": "\n6.1 Overview of computational experiments with simulated data",
    "text": "6.1 Overview of computational experiments with simulated data\nIn Chapter 5 we discussed running computational experiments with empirical datasets as the experimental units in our treatment comparisons. What advantages might there be in running the experiments with simulated data as the experimental units? Let’s consider this idea within the same construct of our experiments on variable selection between stepwise and lasso methods.\nWith the previous computational experiment, we had a list of 9 datasets. This is not the most exhaustive list to be trying a timing study on for example. If we have to find a new, real dataset everytime we want time our new method, then we are going to have to find lots of new datasets. And all would have an uncontrolled number of variables and rows, which doesn’t generalize well (e.g. sample size isn’t growing in orders of magnitude, so can’t make comparisons like X times longer when 2 times more rows). This is not a very systematic way of searching (just finding some random datasets will lead to not learning anything about timing if all are tiny).\nInstead of trying to find the perfect dataset that matches all the characteristics we want, we might want to build a dataset. Then we can set the sample size and the number of variables, and study if our stepwise selection method’s timing more a function of \\(n\\) or \\(p\\) (and the number of actually important (significant, related to \\(Y\\)) covariates \\(q\\) (\\(q &lt; p\\))). There’s lots of things that go into the timing, and relying on finding a dataset where I know all of these things as true is nrealy impossible.\nWhy choose simulated vs. real datasets for experiments?\n\n\nAdvantages of simulated data:\n\nI can control the “true” parameters that create the data (\\(q\\)), which is greatly preferred when evaluating the number of true parameters that tend to be chosen\nI can control the sample sizes (\\(n\\)) and the number of variables (\\(p\\)), which is a hige advantage in timing studies (i.e. easier to capture timing info in a simulated environment)\nSimulating data has almost zero cost to uses bigger and bigger datasets (just computational resources)\n\n\n\nAdvantages of real data:\n\n\nPredictive accuracy is measured more realistically; if wanting to study how methods perform in practice, we should have them perform in practice and not in just theory (i.e. a well-controlled, simulated environment)\n\nNot going to get the oddities of real data such as outliers unless they are specifically coded in\n\n\n\n\n\nHow might we choose to simulate data to conduct the experiments for the variable selection above?",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "simulation-studies.html#simulating-data-for-your-experiment",
    "href": "simulation-studies.html#simulating-data-for-your-experiment",
    "title": "6  Simulation Studies",
    "section": "\n6.2 Simulating data for your experiment",
    "text": "6.2 Simulating data for your experiment\nWhen we are building our simulation study, we need to decide on how the data is generated, how we make the process reproducible and if we need to store the simulated datasets to allow for future audit.\nSimulate data function: Create a function that will simulate the x-values for our regression variable selection study.\n\n\nLet \\(Y = X \\beta + \\epsilon\\), where \\(\\epsilon \\sim \\text{Normal}(0, \\sigma)\\)\n\nSimulating from independent normal random variables (very simply), which lines up with the assumptions of LASSO. LASSO is basically fitting an MLR model with independent normal structure and some normal noise.\n\n\n\nParameters\n\n\nn = number of simulated rows\n\np = number of simulated covariates (X)\n\nq = number of covariates linearly related to Y\n\nb = strength of beta coefficients for linearly related X’s\n\nCan have stronger relationships with higher magnitudes of betas (such as 0.1 vs 10 vs 50)\nThis could be used to see if weak relationships are findable by the variable selection method\n\n\n\nsd_y = sd of simulated y values\n\nsd_x = sd of simulated x variables\n\n\n\n\n# define function to simulate response and covariates\nmake_sim_data &lt;- function(n = 100, p = 20, q = 10, b = 0.1, sd_y = 1, sd_x = 1){\n  \n  # generate covariates\n  X = sapply(1:p, function(i) rnorm(n, mean = 0, sd = sd_x))\n  \n  # give column names\n  colnames(X) = paste0(\"x\", 1:p)\n  \n  # generate beta vector (q significant, non-zero parameters and p-q zeros)\n  beta = c(rep(b, q), rep(0, p-q))\n  \n  # calculate response\n  y = (X %*% beta)[,1] + rnorm(n, mean = 0, sd = sd_y)\n  \n  # save as datafram\n  data_sim = data.frame(y,X)\n  \n  return(data_sim)\n  \n}\n\n# get the random same data each time\nset.seed(12345)\n\n# test function\nmake_sim_data(n = 10, p = 2, q = 2, b = 0.1, sd_y = 1, sd_x = 1)\n\n            y         x1         x2\n1   0.8265500  0.5855288 -0.1162478\n2   1.7084629  0.7094660  1.8173120\n3  -0.6181960 -0.1093033  0.3706279\n4  -1.5464655 -0.4534972  0.5202165\n5  -1.6121740  0.6058875 -0.7505320\n6   1.7049919 -1.8179560  0.8168998\n7  -0.5072733  0.6300986 -0.8863575\n8   0.5596036 -0.2761841 -0.3315776\n9   0.6957788 -0.2841597  1.1207127\n10 -0.2243708 -0.9193220  0.2987237\n\n\nWe know lots of information about the simulated dataset this function returns and how it should behave in a linear model. For example, if studying variable selection, we want x1:xq to be selected in the final model.\nSo, we can now really quickly build any number of datasets that we want with the desired attributes.\nset.seed() tangent\n\nThis is supposed to run setup the psuedo random number generators start at the same point. So if the code is deterministic, then it should run in the same way\nCan be tricky with functions because if introducing more randomness, it changes the position of the psuedo random generator. So everything is thrown off",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "simulation-studies.html#running-the-simulation-study",
    "href": "simulation-studies.html#running-the-simulation-study",
    "title": "6  Simulation Studies",
    "section": "\n6.3 Running the simulation study",
    "text": "6.3 Running the simulation study\nHere are the final helper functions from Chapter 5.\n\n# load packages\nlibrary(glmnet)\nlibrary(caret)\n\n# function for choosing with stepwise and fitting a regression\n# -&gt; inputs dataframe and returns selected model\nstep_var_mod &lt;- function(df){\n  \n  # run stepwise procedure from full model\n  step_selected &lt;- step(lm(y ~ . , data = df), trace = FALSE)\n  \n  return(step_selected)\n\n}\n# function for choosing with lasso and fitting regression\n# -&gt; inputs dataframe and returns selected model\nlasso_var_mod &lt;- function(df){\n  \n  # tune shrinkage parameter lambda\n  cv.out &lt;- cv.glmnet(x = as.matrix(x = df[ , -which(names(df) == \"y\")]),\n                      y = df$y, alpha = 1, type.measure = \"deviance\")\n  \n  # run lasso selection on model using tuned lambda\n  lasso_mod &lt;- glmnet(x = as.matrix(x = df[ , -which(names(df) == \"y\")]),\n                      y = df$y, alpha = 1, lambda = cv.out$lambda.1se)\n  \n  # save names of non-shrunk X variables\n  lasso_vars &lt;- names(lasso_mod$beta[,1])[which(lasso_mod$beta[,1] != 0)]\n  \n  # HACK (not elegant solution): lasso had a tendency to select zero variables which breaks the timing study below\n  # -&gt; so if no variables are selected, just take the first variable\n  if (length(lasso_vars) == 0)\n    lasso_vars &lt;- names(lasso_mod$beta[,1])[1]\n  \n  # fit model based on lasso selected variables (plus intercept)\n  lasso_selected &lt;- lm(formula(paste0(\"y ~ 1 + \", paste(lasso_vars, collapse = \" + \"))), data = df)\n  \n  return(lasso_selected)\n\n}\n\n# function for finding number of variables included\n# -&gt; inputs a model and returns an integer\nselect_var_count &lt;- function(lin_mod){\n  \n  # count the number of variables in the model (excluding intercept)\n  length(coef(lin_mod))-1\n  \n}\n\n# function for finding 10-fold cross validated RMSE (our accuracy measure)\nselect_cv_rmse &lt;- function(lin_mod){\n  \n  # run 10-fold CV on the model\n  # -&gt; by default trainControl() uses bootstrap validation, so need to switch it\n  # -&gt; always want to use intercept, else it will try to tune the intercept (decide to include or not include it), the stepwise always gives an intercept so need fair comparison\n  cv_result &lt;- train(formula(lin_mod), \n                     data = lin_mod$model,\n                     method = \"lm\",\n                     trControl = trainControl(method = \"cv\", number = 10),\n                     tuneGrid = data.frame(intercept = TRUE))\n  \n  # return RMSE\n  return(cv_result$results$RMSE)\n}\n\n# define function to run a single trial\n# -&gt; inputs each subject (df), applies the treatment (selection_alg which is a function), and collects the results\nrun_trial &lt;- function(selection_alg, df) {\n  \n  # run variable selection model\n  # -&gt; we can use a tmp prefix for the model to represent a temporary object (model) (it is temporary because it is in a temporary environment when the function is called)\n  # record start and end time\n  start_time = Sys.time()\n    tmp_mod = selection_alg(df)\n  end_time = Sys.time()\n  \n  # collect measurements for number of variables and predictive accuracy\n  # -&gt; will be storing results as dataframe, so want to return a mini dataframe here\n  # -&gt; want to name elements when returning more complex data structures\n  return(data.frame(nvars = select_var_count(tmp_mod),\n                    rmse = select_cv_rmse(tmp_mod),\n                    time = difftime(end_time, start_time, units = \"sec\")))\n}\n\n\n6.3.1 Timing study\nTo help with the timing study, we will use the tictoc package. tic() (starts the timer) / toc() (records the time to get the toc()) pairing does the data collection of setting up a start_time &lt;- Sys.time() and stop_time &lt;- Sys.time() for us in an automated way and creates a log file.\nWe want to be careful about where we put the tic() and toc(): we don’t care about how long it takes to generate the data, rather just the variable selection method. So we can run a loop with constant data dimensions to get an idea of the variability of the selection method timing.\nThere are certain combinations that we are interested in checking. Stepwise method does a lot of solving the inverse of the \\(X'X\\) matrix (a \\(p \\times p\\) matrix), which is the time consuming part. So if \\(p = 10\\) is small, maybe stepwise is faster than lasso (maybe with less variability) but as \\(p\\) increases, perhaps lasso becomes quicker. Can collect this information and perform inferences on the times (systematic effect of changing \\(n\\), \\(p\\), \\(q\\) just like any other experiment (these are the treatments, in addition to the selection algorithm).\n\n# load package\nlibrary(tictoc)\nlibrary(magrittr)\n\n# demonstrate tictoc timers\n# -&gt; don't care about the parameters other than dimensions, so just use the defaults\ntic.clearlog()\nfor(i in 1:3){\n  \n  # generate data\n  data_sim &lt;- make_sim_data(n = 50000, p = 10, q = 5)\n  \n  # time variable selection\n  tic(paste0(\"sim\",i))\n    run_trial(lasso_var_mod, data_sim)\n  toc(log = TRUE, quiet = TRUE)\n}\n\n# get information from tictoc\n# -&gt; then calculate differences and simplify\nlog_lst &lt;- tic.log(format = FALSE)\ntimings &lt;- lapply(log_lst, function(x) x$toc - x$tic) %&gt;% unlist\ntimings\n\nelapsed elapsed elapsed \n  2.534   2.178   2.073 \n\n\nNow we can generalize the above code for any \\(n\\), \\(p\\) and \\(q\\), which are the things that we want to systematically change.\n\n# define function to run a timing study\n# -&gt; default values are picked so that if we accidentally run the function, it wont take forever\n# -&gt; we can also make it more flexible to be able to do different model selection functions\nsim_times &lt;- function(n_sims = 10, n = 100, p = 10, q = 5, var_select_fun = step_var_mod){\n  \n  tic.clearlog()\n  for(i in 1:n_sims){\n    \n    # generate data\n    data_sim &lt;- make_sim_data(n = n, p = p, q = q)\n    \n    # run and time variable selection\n    # -&gt; ignoring the outputs of run_trial() because just want the timing info\n    tic(paste0(\"sim\",i))\n      run_trial(var_select_fun, data_sim)\n    toc(log = TRUE, quiet = TRUE)\n  }\n  \n  # get information from tictoc\n  # -&gt; then calculate differences and simplify\n  log_lst &lt;- tic.log(format = FALSE)\n  timings &lt;- lapply(log_lst, function(x) x$toc - x$tic) %&gt;% unlist\n\n  return(timings)\n}\n\n# test function to see as when we had it hardcoded for a specific set of parameters\nsim_times(n_sims = 3, n = 50000, p = 10, q = 5, var_select_fun = lasso_var_mod)\n\nelapsed elapsed elapsed \n  2.228   2.185   2.140 \n\n# now much easier to change the parameter values\nsim_times(n_sims = 3, n = 500, p = 10, q = 5, var_select_fun = step_var_mod)\n\nelapsed elapsed elapsed \n  0.128   0.144   0.118 \n\n\nNow we have a function that can perform a timing study for a particular parameter set. But we want to use it in a way that we can go through a grid of parameter combinations.\nmapply() iterates through vectors of the same length (which is what each column of a dataframe is; it is looking for (vector1[i], vector2[i]) which we will create via expand.grid() to give us all possible combinations.\n\n# define possible parameter combinations\nparams &lt;- expand.grid(n = c(100, 1000, 5000), p = c(20, 30), q = c(5, 10))\n\n# run timing study\n# -&gt; mapply() -&gt; MoreArgs are constants\n# -&gt; can't use purrr cause vectorizing over more than two parameters\nstep_times &lt;- mapply(sim_times, n = params$n, p = params$p, q = params$q,\n                     MoreArgs = list(n_sim = 3, var_select_fun = step_var_mod))\nstep_times\n\n         [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11] [,12]\nelapsed 0.165 0.512 2.219 0.296 1.456 6.648 0.150 0.462 1.917 0.285 1.373 6.092\nelapsed 0.153 0.512 2.164 0.281 1.478 6.453 0.156 0.466 2.065 0.288 1.356 6.095\nelapsed 0.147 0.508 2.122 0.277 1.478 6.712 0.152 0.498 2.083 0.337 1.418 6.431\n\n\nNow we can use add the results to the parameters. This is where we use summary statistics such as means, and quantiles to evaluate the timing for each parameter combination.\n\n# add timing results to parameter combination information\nparams$time_mean &lt;- apply(X = step_times, MARGIN = 2, FUN = mean)\nparams\n\n      n  p  q time_mean\n1   100 20  5 0.1550000\n2  1000 20  5 0.5106667\n3  5000 20  5 2.1683333\n4   100 30  5 0.2846667\n5  1000 30  5 1.4706667\n6  5000 30  5 6.6043333\n7   100 20 10 0.1526667\n8  1000 20 10 0.4753333\n9  5000 20 10 2.0216667\n10  100 30 10 0.3033333\n11 1000 30 10 1.3823333\n12 5000 30 10 6.2060000\n\n\n\n6.3.2 Variable selection study\nNow want to look at the number of variables selected rather than the timing. Just have to modify our sim_times() function into a vars_selected() function to get the variables selected (we can compare these results to the respective \\(q\\)’s (the true number of important predictors) that we set).\n\n# define function to run a study on number of variables selected\nvars_selected &lt;- function(n_sims = 10, n = 100, p = 10, q = 5, var_select_fun = step_var_mod){\n  \n  # initialize results vector\n  q_hat = rep(NA, n_sims)\n  \n  for(i in 1:n_sims){\n    \n    # generate data\n    data_sim = make_sim_data(n = n, p = p, q = q)\n    \n    # run variable selection and return jus the number of variables selected\n    q_hat[i] = run_trial(var_select_fun, data_sim)$nvars\n  }\n  \n  return(q_hat)\n}\n\n# test function\n\n# define possible parameter combinations\nparams &lt;- expand.grid(n = c(100, 1000, 5000), p = c(20, 30), q = c(5, 10))\n\n# run variable selection study\nvar_counts &lt;- mapply(vars_selected, n = params$n, p = params$p, q = params$q,\n                     MoreArgs = list(n_sim = 3 , var_select_fun = step_var_mod))\nvar_counts\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n[1,]    4    5    6   12    8    8    8   12   10    10    11    13\n[2,]    3    7    9    9    9    7    9   11   12    15    14    12\n[3,]    4    7    7   12    7    8    4   10   12     6    15    13\n\n# add number of variable results to parameter combination information\nparams$vars_selected &lt;- apply(X = var_counts, MARGIN = 2, FUN = mean)\nparams\n\n      n  p  q vars_selected\n1   100 20  5      3.666667\n2  1000 20  5      6.333333\n3  5000 20  5      7.333333\n4   100 30  5     11.000000\n5  1000 30  5      8.000000\n6  5000 30  5      7.666667\n7   100 20 10      7.000000\n8  1000 20 10     11.000000\n9  5000 20 10     11.333333\n10  100 30 10     10.333333\n11 1000 30 10     13.333333\n12 5000 30 10     12.666667\n\n\nFinal thoughts: In practice, it may be a good idea to save the datasets that were used to get the results as well. These could then be put into a list and saved as a RData file, which could be run off of in the future. This would be very helpful auditing your code and making it more reproducable. But if we are just using simulation to kind of get an idea how the models work, the actual simulated data may not be important and we can just keep the results.",
    "crumbs": [
      "Course Notes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  }
]